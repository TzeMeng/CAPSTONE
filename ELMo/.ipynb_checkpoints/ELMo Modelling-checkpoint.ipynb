{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "established-elite",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from scipy import spatial\n",
    "import torch\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.layers import Embedding\n",
    "from keras.initializers import Constant\n",
    "\n",
    "from keras.layers import Dense, Dropout, Activation, Input\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "technological-madagascar",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: simple_elmo in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from simple_elmo) (2.10.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from simple_elmo) (1.2.2)\n",
      "Requirement already satisfied: smart-open>1.8.1 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from simple_elmo) (3.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from simple_elmo) (1.19.5)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from smart-open>1.8.1->simple_elmo) (2.25.1)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from h5py->simple_elmo) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from pandas->simple_elmo) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from pandas->simple_elmo) (2021.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from requests->smart-open>1.8.1->simple_elmo) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from requests->smart-open>1.8.1->simple_elmo) (1.26.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from requests->smart-open>1.8.1->simple_elmo) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from requests->smart-open>1.8.1->simple_elmo) (4.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade simple_elmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "straight-principle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import training dataset\n",
    "#train = open('data/train/train_QA.xlsx',)\n",
    "train_QA = pd.read_excel(r'data/train/train_QA.xlsx')\n",
    "train_context = pd.read_excel(r'data/train/train_context.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "changing-repair",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>contextID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>North Carolina consists of three main geograph...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The coastal plain transitions to the Piedmont ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The western section of the state is part of th...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The climate of the coastal plain is influenced...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Atlantic Ocean has less influence on the c...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  contextID\n",
       "0  North Carolina consists of three main geograph...          1\n",
       "1  The coastal plain transitions to the Piedmont ...          2\n",
       "2  The western section of the state is part of th...          3\n",
       "3  The climate of the coastal plain is influenced...          4\n",
       "4  The Atlantic Ocean has less influence on the c...          5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_context.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "scientific-index",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('options.json', <http.client.HTTPMessage at 0x1df7c5d8df0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_5.5B/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json', 'options.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "living-cattle",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-16 17:45:45,733 : INFO : Loading model from elmopath...\n",
      "2021-02-16 17:45:45,735 : INFO : No vocabulary file found in the model.\n",
      "2021-02-16 17:45:45,737 : INFO : No vocabulary file provided; using special tokens only.\n",
      "2021-02-16 17:45:45,738 : INFO : We will cache the vocabulary of 3 tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\anaconda3\\envs\\Capstone\\lib\\site-packages\\simple_elmo\\model.py:570: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:903: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  warnings.warn(\"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \"\n",
      "2021-02-16 17:45:47,264 : WARNING : From C:\\Users\\User\\anaconda3\\envs\\Capstone\\lib\\site-packages\\simple_elmo\\model.py:570: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:981: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1727: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n",
      "2021-02-16 17:45:47,795 : WARNING : From C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:981: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The model is now loaded.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from simple_elmo import ElmoModel\n",
    "model = ElmoModel()\n",
    "model.load(r'elmopath')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "collective-windsor",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['I love potatoes', 'Tzemeng loves mala', 'Hazel loves Xiao', 'xingying loves anime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "typical-efficiency",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-16 21:53:53,856 : INFO : Warming up ELMo on 4 sentences...\n",
      "2021-02-16 21:53:56,187 : INFO : Warming up finished.\n",
      "2021-02-16 21:53:56,188 : INFO : Texts in the current batch: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[-0.19745192, -0.41884196, -0.07295133, ..., -0.2019807 ,\n",
       "         -0.11709898,  0.21184988],\n",
       "        [-0.02613379, -0.33026373,  0.31403264, ..., -0.02769373,\n",
       "         -0.03423417,  0.05347976],\n",
       "        [ 0.05580795, -0.45201886,  0.46087283, ...,  0.16257262,\n",
       "         -0.30399969,  0.17682366],\n",
       "        ...,\n",
       "        [ 0.00173837, -0.01513954,  0.05382365, ...,  0.04019766,\n",
       "         -0.07493153,  0.00220637],\n",
       "        [ 0.00173837, -0.01513954,  0.05382365, ...,  0.04019766,\n",
       "         -0.07493153,  0.00220637],\n",
       "        [ 0.00173837, -0.01513954,  0.05382365, ...,  0.04019766,\n",
       "         -0.07493153,  0.00220637]],\n",
       "\n",
       "       [[-0.62830472, -0.31021583,  0.48361775, ...,  0.07993049,\n",
       "         -0.27295381,  0.27352491],\n",
       "        [-0.5490095 , -0.28553104,  0.6998781 , ...,  0.01919892,\n",
       "         -0.36845356,  0.1718657 ],\n",
       "        [-0.3555795 ,  0.06677981,  0.37380549, ..., -0.06110012,\n",
       "         -0.45065731,  0.18366686],\n",
       "        ...,\n",
       "        [-0.11399087, -0.69189274,  0.29673722, ..., -0.21818581,\n",
       "         -0.31769109,  0.10352661],\n",
       "        [ 0.00173837, -0.01513954,  0.05382365, ...,  0.04019766,\n",
       "         -0.07493153,  0.00220637],\n",
       "        [ 0.00173837, -0.01513954,  0.05382365, ...,  0.04019766,\n",
       "         -0.07493153,  0.00220637]],\n",
       "\n",
       "       [[-0.36804533, -0.25667924,  0.0156424 , ..., -0.10074968,\n",
       "         -0.37100786,  0.36546057],\n",
       "        [-0.20788147, -1.02089369,  0.58674145, ..., -0.15905765,\n",
       "         -0.63146102,  0.23863101],\n",
       "        [-0.18462673, -0.0643319 ,  0.48617443, ..., -0.10709909,\n",
       "         -0.42204535,  0.17395781],\n",
       "        ...,\n",
       "        [ 0.00173837, -0.01513954,  0.05382365, ...,  0.04019766,\n",
       "         -0.07493153,  0.00220637],\n",
       "        [ 0.00173837, -0.01513954,  0.05382365, ...,  0.04019766,\n",
       "         -0.07493153,  0.00220637],\n",
       "        [ 0.00173837, -0.01513954,  0.05382365, ...,  0.04019766,\n",
       "         -0.07493153,  0.00220637]],\n",
       "\n",
       "       [[-0.44570097, -0.37916979,  0.52824551, ...,  0.20440412,\n",
       "         -0.42534527,  0.23011769],\n",
       "        [-0.51886225, -0.2717886 ,  0.71673352, ...,  0.03298841,\n",
       "         -0.39310831,  0.33695346],\n",
       "        [-0.15772025,  0.28256828,  0.28172871, ...,  0.51234496,\n",
       "         -0.39021033,  0.35099506],\n",
       "        ...,\n",
       "        [-0.26662689, -0.32508624,  0.5203504 , ..., -0.16040064,\n",
       "         -0.11649157,  0.19642417],\n",
       "        [-0.16506952, -0.0139496 ,  0.08872951, ..., -0.42552707,\n",
       "         -0.1492946 , -0.09809762],\n",
       "        [-0.15972859,  0.06283095,  0.42403573, ..., -0.40038377,\n",
       "         -0.22677623,  0.04326809]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The get_elmo_vectors() method produces a tensor of contextualized word embeddings. \n",
    "# Its shape is (number of sentences, the length of the longest sentence, ELMo dimensionality).\n",
    "model.get_elmo_vectors(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dried-edward",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-16 22:23:22,008 : INFO : Warming up ELMo on 4 sentences...\n",
      "2021-02-16 22:23:22,801 : INFO : Warming up finished.\n",
      "2021-02-16 22:23:22,803 : INFO : Texts in the current batch: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.01197572, -0.03256201,  0.05645592, ..., -0.00174162,\n",
       "        -0.03306543,  0.02126458],\n",
       "       [-0.0231729 , -0.02101358,  0.04614951, ...,  0.0109503 ,\n",
       "        -0.05248468,  0.01727588],\n",
       "       [-0.02600693, -0.03952937,  0.06087819, ..., -0.00616964,\n",
       "        -0.06249807,  0.02318627],\n",
       "       [-0.0298245 , -0.01185991,  0.04964055, ...,  0.01127442,\n",
       "        -0.04647353,  0.02058413]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_elmo_vector_average(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ultimate-updating",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: allennlp in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (1.4.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "en-core-web-sm 3.0.0 requires spacy<3.1.0,>=3.0.0, but you have spacy 2.3.5 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting allennlp\n",
      "  Using cached allennlp-2.0.1-py3-none-any.whl (580 kB)\n",
      "Requirement already satisfied: pytest in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from allennlp) (6.2.2)\n",
      "Requirement already satisfied: requests>=2.18 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from allennlp) (2.25.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from allennlp) (1.6.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from allennlp) (0.1.91)\n",
      "Requirement already satisfied: filelock<3.1,>=3.0 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from allennlp) (3.0.12)\n",
      "Requirement already satisfied: boto3<2.0,>=1.14 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from allennlp) (1.17.8)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from allennlp) (1.19.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from allennlp) (0.24.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from allennlp) (3.5)\n",
      "  Using cached allennlp-2.0.0-py3-none-any.whl (579 kB)\n",
      "Requirement already satisfied: tqdm>=4.19 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from allennlp) (4.56.1)\n",
      "Requirement already satisfied: transformers<4.3,>=4.1 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from allennlp) (4.2.2)\n",
      "Requirement already satisfied: h5py in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from allennlp) (2.10.0)\n",
      "Requirement already satisfied: torch<1.8.0,>=1.6.0 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from allennlp) (1.7.1)\n",
      "Requirement already satisfied: overrides==3.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from allennlp) (3.1.0)\n",
      "Requirement already satisfied: tensorboardX>=1.2 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from allennlp) (2.1)\n",
      "Requirement already satisfied: jsonpickle in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from allennlp) (2.0.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from boto3<2.0,>=1.14->allennlp) (0.3.4)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.8 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from boto3<2.0,>=1.14->allennlp) (1.20.8)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from boto3<2.0,>=1.14->allennlp) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from botocore<1.21.0,>=1.20.8->boto3<2.0,>=1.14->allennlp) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from botocore<1.21.0,>=1.20.8->boto3<2.0,>=1.14->allennlp) (1.26.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.8->boto3<2.0,>=1.14->allennlp) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from requests>=2.18->allennlp) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from requests>=2.18->allennlp) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from requests>=2.18->allennlp) (4.0.0)\n",
      "Collecting spacy<2.4,>=2.1.0\n",
      "  Using cached spacy-2.3.5-cp38-cp38-win_amd64.whl (9.7 MB)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from spacy<2.4,>=2.1.0->allennlp) (0.7.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from spacy<2.4,>=2.1.0->allennlp) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from spacy<2.4,>=2.1.0->allennlp) (2.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from spacy<2.4,>=2.1.0->allennlp) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from spacy<2.4,>=2.1.0->allennlp) (3.0.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from spacy<2.4,>=2.1.0->allennlp) (52.0.0.post20210125)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from spacy<2.4,>=2.1.0->allennlp) (0.8.2)\n",
      "Collecting catalogue<1.1.0,>=0.0.7\n",
      "  Using cached catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Collecting srsly<1.1.0,>=1.0.2\n",
      "  Using cached srsly-1.0.5-cp38-cp38-win_amd64.whl (178 kB)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from tensorboardX>=1.2->allennlp) (3.14.0)\n",
      "Collecting thinc<7.5.0,>=7.4.1\n",
      "  Using cached thinc-7.4.5-cp38-cp38-win_amd64.whl (910 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from torch<1.8.0,>=1.6.0->allennlp) (3.7.4.3)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from transformers<4.3,>=4.1->allennlp) (0.9.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from transformers<4.3,>=4.1->allennlp) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from transformers<4.3,>=4.1->allennlp) (2020.11.13)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from transformers<4.3,>=4.1->allennlp) (0.0.43)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from nltk->allennlp) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from nltk->allennlp) (1.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from packaging->transformers<4.3,>=4.1->allennlp) (2.4.7)\n",
      "Requirement already satisfied: pluggy<1.0.0a1,>=0.12 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from pytest->allennlp) (0.13.1)\n",
      "Requirement already satisfied: toml in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from pytest->allennlp) (0.10.2)\n",
      "Requirement already satisfied: py>=1.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from pytest->allennlp) (1.10.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from pytest->allennlp) (1.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from pytest->allennlp) (0.4.4)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (from pytest->allennlp) (1.1.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from pytest->allennlp) (20.3.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from scikit-learn->allennlp) (2.1.0)\n",
      "Installing collected packages: srsly, catalogue, thinc, spacy\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.0.1\n",
      "    Uninstalling thinc-8.0.1:\n",
      "      Successfully uninstalled thinc-8.0.1\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.0.3\n",
      "    Uninstalling spacy-3.0.3:\n",
      "      Successfully uninstalled spacy-3.0.3\n",
      "Successfully installed catalogue-1.0.0 spacy-2.3.5 srsly-1.0.5 thinc-7.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip install --user --upgrade allennlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "indonesian-acceptance",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\user\\appdata\\roaming\\python\\python38\\site-packages (2.3.5)\n",
      "Collecting spacy\n",
      "  Using cached spacy-3.0.3-cp38-cp38-win_amd64.whl (11.8 MB)\n",
      "Requirement already satisfied: pathy in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from spacy) (0.3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from spacy) (1.19.5)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from spacy) (1.7.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from spacy) (2.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from spacy) (3.0.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from spacy) (0.7.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from spacy) (4.56.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from spacy) (52.0.0.post20210125)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Collecting thinc<8.1.0,>=8.0.0\n",
      "  Using cached thinc-8.0.1-cp38-cp38-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in c:\\users\\user\\anaconda3\\envs\\capstone\\lib\\site-packages (from pathy->spacy) (3.0.0)\n",
      "Installing collected packages: thinc, spacy\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 7.4.5\n",
      "    Uninstalling thinc-7.4.5:\n",
      "      Successfully uninstalled thinc-7.4.5\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 2.3.5\n",
      "    Uninstalling spacy-2.3.5:\n",
      "      Successfully uninstalled spacy-2.3.5\n",
      "Successfully installed spacy-3.0.3 thinc-8.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script spacy.exe is installed in 'C:\\Users\\User\\AppData\\Roaming\\Python\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "allennlp 1.4.1 requires spacy<2.4,>=2.1.0, but you have spacy 3.0.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install --user --upgrade spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-packing",
   "metadata": {},
   "source": [
    "Experimenting with BILM from ALLENNLP repo\n",
    "https://github.com/allenai/bilm-tf/tree/master/bilm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "disturbed-ending",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/allenai/bilm-tf/blob/master/bilm/model.py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import json\n",
    "import re\n",
    "\n",
    "from ipynb.fs.full.elmodata import UnicodeCharsVocabulary, Batcher\n",
    "\n",
    "DTYPE = 'float32'\n",
    "DTYPE_INT = 'int64'\n",
    "\n",
    "\n",
    "class BidirectionalLanguageModel(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            options_file: str,\n",
    "            weight_file: str,\n",
    "            use_character_inputs=True,\n",
    "            embedding_weight_file=None,\n",
    "            max_batch_size=128,\n",
    "        ):\n",
    "        '''\n",
    "        Creates the language model computational graph and loads weights\n",
    "        Two options for input type:\n",
    "            (1) To use character inputs (paired with Batcher)\n",
    "                pass use_character_inputs=True, and ids_placeholder\n",
    "                of shape (None, None, max_characters_per_token)\n",
    "                to __call__\n",
    "            (2) To use token ids as input (paired with TokenBatcher),\n",
    "                pass use_character_inputs=False and ids_placeholder\n",
    "                of shape (None, None) to __call__.\n",
    "                In this case, embedding_weight_file is also required input\n",
    "        options_file: location of the json formatted file with\n",
    "                      LM hyperparameters\n",
    "        weight_file: location of the hdf5 file with LM weights\n",
    "        use_character_inputs: if True, then use character ids as input,\n",
    "            otherwise use token ids\n",
    "        max_batch_size: the maximum allowable batch size \n",
    "        '''\n",
    "        with open(options_file, 'r') as fin:\n",
    "            options = json.load(fin)\n",
    "\n",
    "        if not use_character_inputs:\n",
    "            if embedding_weight_file is None:\n",
    "                raise ValueError(\n",
    "                    \"embedding_weight_file is required input with \"\n",
    "                    \"not use_character_inputs\"\n",
    "                )\n",
    "\n",
    "        self._options = options\n",
    "        self._weight_file = weight_file\n",
    "        self._embedding_weight_file = embedding_weight_file\n",
    "        self._use_character_inputs = use_character_inputs\n",
    "        self._max_batch_size = max_batch_size\n",
    "\n",
    "        self._ops = {}\n",
    "        self._graphs = {}\n",
    "\n",
    "    def __call__(self, ids_placeholder):\n",
    "        '''\n",
    "        Given the input character ids (or token ids), returns a dictionary\n",
    "            with tensorflow ops:\n",
    "            {'lm_embeddings': embedding_op,\n",
    "             'lengths': sequence_lengths_op,\n",
    "             'mask': op to compute mask}\n",
    "        embedding_op computes the LM embeddings and is shape\n",
    "            (None, 3, None, 1024)\n",
    "        lengths_op computes the sequence lengths and is shape (None, )\n",
    "        mask computes the sequence mask and is shape (None, None)\n",
    "        ids_placeholder: a tf.placeholder of type int32.\n",
    "            If use_character_inputs=True, it is shape\n",
    "                (None, None, max_characters_per_token) and holds the input\n",
    "                character ids for a batch\n",
    "            If use_character_input=False, it is shape (None, None) and\n",
    "                holds the input token ids for a batch\n",
    "        '''\n",
    "        if ids_placeholder in self._ops:\n",
    "            # have already created ops for this placeholder, just return them\n",
    "            ret = self._ops[ids_placeholder]\n",
    "\n",
    "        else:\n",
    "            # need to create the graph\n",
    "            if len(self._ops) == 0:\n",
    "                # first time creating the graph, don't reuse variables\n",
    "                lm_graph = BidirectionalLanguageModelGraph(\n",
    "                    self._options,\n",
    "                    self._weight_file,\n",
    "                    ids_placeholder,\n",
    "                    embedding_weight_file=self._embedding_weight_file,\n",
    "                    use_character_inputs=self._use_character_inputs,\n",
    "                    max_batch_size=self._max_batch_size)\n",
    "            else:\n",
    "                with tf.variable_scope('', reuse=True):\n",
    "                    lm_graph = BidirectionalLanguageModelGraph(\n",
    "                        self._options,\n",
    "                        self._weight_file,\n",
    "                        ids_placeholder,\n",
    "                        embedding_weight_file=self._embedding_weight_file,\n",
    "                        use_character_inputs=self._use_character_inputs,\n",
    "                        max_batch_size=self._max_batch_size)\n",
    "\n",
    "            ops = self._build_ops(lm_graph)\n",
    "            self._ops[ids_placeholder] = ops\n",
    "            self._graphs[ids_placeholder] = lm_graph\n",
    "            ret = ops\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def _build_ops(self, lm_graph):\n",
    "        with tf.control_dependencies([lm_graph.update_state_op]):\n",
    "            # get the LM embeddings\n",
    "            token_embeddings = lm_graph.embedding\n",
    "            layers = [\n",
    "                tf.concat([token_embeddings, token_embeddings], axis=2)\n",
    "            ]\n",
    "\n",
    "            n_lm_layers = len(lm_graph.lstm_outputs['forward'])\n",
    "            for i in range(n_lm_layers):\n",
    "                layers.append(\n",
    "                    tf.concat(\n",
    "                        [lm_graph.lstm_outputs['forward'][i],\n",
    "                         lm_graph.lstm_outputs['backward'][i]],\n",
    "                        axis=-1\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # The layers include the BOS/EOS tokens.  Remove them\n",
    "            sequence_length_wo_bos_eos = lm_graph.sequence_lengths - 2\n",
    "            layers_without_bos_eos = []\n",
    "            for layer in layers:\n",
    "                layer_wo_bos_eos = layer[:, 1:, :]\n",
    "                layer_wo_bos_eos = tf.reverse_sequence(\n",
    "                    layer_wo_bos_eos, \n",
    "                    lm_graph.sequence_lengths - 1,\n",
    "                    seq_axis=1,\n",
    "                    batch_axis=0,\n",
    "                )\n",
    "                layer_wo_bos_eos = layer_wo_bos_eos[:, 1:, :]\n",
    "                layer_wo_bos_eos = tf.reverse_sequence(\n",
    "                    layer_wo_bos_eos,\n",
    "                    sequence_length_wo_bos_eos,\n",
    "                    seq_axis=1,\n",
    "                    batch_axis=0,\n",
    "                )\n",
    "                layers_without_bos_eos.append(layer_wo_bos_eos)\n",
    "\n",
    "            # concatenate the layers\n",
    "            lm_embeddings = tf.concat(\n",
    "                [tf.expand_dims(t, axis=1) for t in layers_without_bos_eos],\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "            # get the mask op without bos/eos.\n",
    "            # tf doesn't support reversing boolean tensors, so cast\n",
    "            # to int then back\n",
    "            mask_wo_bos_eos = tf.cast(lm_graph.mask[:, 1:], 'int32')\n",
    "            mask_wo_bos_eos = tf.reverse_sequence(\n",
    "                mask_wo_bos_eos,\n",
    "                lm_graph.sequence_lengths - 1,\n",
    "                seq_axis=1,\n",
    "                batch_axis=0,\n",
    "            )\n",
    "            mask_wo_bos_eos = mask_wo_bos_eos[:, 1:]\n",
    "            mask_wo_bos_eos = tf.reverse_sequence(\n",
    "                mask_wo_bos_eos,\n",
    "                sequence_length_wo_bos_eos,\n",
    "                seq_axis=1,\n",
    "                batch_axis=0,\n",
    "            )\n",
    "            mask_wo_bos_eos = tf.cast(mask_wo_bos_eos, 'bool')\n",
    "\n",
    "        return {\n",
    "            'lm_embeddings': lm_embeddings, \n",
    "            'lengths': sequence_length_wo_bos_eos,\n",
    "            'token_embeddings': lm_graph.embedding,\n",
    "            'mask': mask_wo_bos_eos,\n",
    "        }\n",
    "\n",
    "\n",
    "def _pretrained_initializer(varname, weight_file, embedding_weight_file=None):\n",
    "    '''\n",
    "    We'll stub out all the initializers in the pretrained LM with\n",
    "    a function that loads the weights from the file\n",
    "    '''\n",
    "    weight_name_map = {}\n",
    "    for i in range(2):\n",
    "        for j in range(8):  # if we decide to add more layers\n",
    "            root = 'RNN_{}/RNN/MultiRNNCell/Cell{}'.format(i, j)\n",
    "            weight_name_map[root + '/rnn/lstm_cell/kernel'] = \\\n",
    "                root + '/LSTMCell/W_0'\n",
    "            weight_name_map[root + '/rnn/lstm_cell/bias'] = \\\n",
    "                root + '/LSTMCell/B'\n",
    "            weight_name_map[root + '/rnn/lstm_cell/projection/kernel'] = \\\n",
    "                root + '/LSTMCell/W_P_0'\n",
    "\n",
    "    # convert the graph name to that in the checkpoint\n",
    "    varname_in_file = varname[5:]\n",
    "    if varname_in_file.startswith('RNN'):\n",
    "        varname_in_file = weight_name_map[varname_in_file]\n",
    "\n",
    "    if varname_in_file == 'embedding':\n",
    "        with h5py.File(embedding_weight_file, 'r') as fin:\n",
    "            # Have added a special 0 index for padding not present\n",
    "            # in the original model.\n",
    "            embed_weights = fin[varname_in_file][...]\n",
    "            weights = np.zeros(\n",
    "                (embed_weights.shape[0] + 1, embed_weights.shape[1]),\n",
    "                dtype=DTYPE\n",
    "            )\n",
    "            weights[1:, :] = embed_weights\n",
    "    else:\n",
    "        with h5py.File(weight_file, 'r') as fin:\n",
    "            if varname_in_file == 'char_embed':\n",
    "                # Have added a special 0 index for padding not present\n",
    "                # in the original model.\n",
    "                char_embed_weights = fin[varname_in_file][...]\n",
    "                weights = np.zeros(\n",
    "                    (char_embed_weights.shape[0] + 1,\n",
    "                     char_embed_weights.shape[1]),\n",
    "                    dtype=DTYPE\n",
    "                )\n",
    "                weights[1:, :] = char_embed_weights\n",
    "            else:\n",
    "                weights = fin[varname_in_file][...]\n",
    "\n",
    "    # Tensorflow initializers are callables that accept a shape parameter\n",
    "    # and some optional kwargs\n",
    "    def ret(shape, **kwargs):\n",
    "        if list(shape) != list(weights.shape):\n",
    "            raise ValueError(\n",
    "                \"Invalid shape initializing {0}, got {1}, expected {2}\".format(\n",
    "                    varname_in_file, shape, weights.shape)\n",
    "            )\n",
    "        return weights\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "class BidirectionalLanguageModelGraph(object):\n",
    "    '''\n",
    "    Creates the computational graph and holds the ops necessary for runnint\n",
    "    a bidirectional language model\n",
    "    '''\n",
    "    def __init__(self, options, weight_file, ids_placeholder,\n",
    "                 use_character_inputs=True, embedding_weight_file=None,\n",
    "                 max_batch_size=128):\n",
    "\n",
    "        self.options = options\n",
    "        self._max_batch_size = max_batch_size\n",
    "        self.ids_placeholder = ids_placeholder\n",
    "        self.use_character_inputs = use_character_inputs\n",
    "\n",
    "        # this custom_getter will make all variables not trainable and\n",
    "        # override the default initializer\n",
    "        def custom_getter(getter, name, *args, **kwargs):\n",
    "            kwargs['trainable'] = False\n",
    "            kwargs['initializer'] = _pretrained_initializer(\n",
    "                name, weight_file, embedding_weight_file\n",
    "            )\n",
    "            return getter(name, *args, **kwargs)\n",
    "\n",
    "        if embedding_weight_file is not None:\n",
    "            # get the vocab size\n",
    "            with h5py.File(embedding_weight_file, 'r') as fin:\n",
    "                # +1 for padding\n",
    "                self._n_tokens_vocab = fin['embedding'].shape[0] + 1\n",
    "        else:\n",
    "            self._n_tokens_vocab = None\n",
    "\n",
    "        with tf.variable_scope('bilm', custom_getter=custom_getter):\n",
    "            self._build()\n",
    "\n",
    "    def _build(self):\n",
    "        if self.use_character_inputs:\n",
    "            self._build_word_char_embeddings()\n",
    "        else:\n",
    "            self._build_word_embeddings()\n",
    "        self._build_lstms()\n",
    "\n",
    "    def _build_word_char_embeddings(self):\n",
    "        '''\n",
    "        options contains key 'char_cnn': {\n",
    "        'n_characters': 262,\n",
    "        # includes the start / end characters\n",
    "        'max_characters_per_token': 50,\n",
    "        'filters': [\n",
    "            [1, 32],\n",
    "            [2, 32],\n",
    "            [3, 64],\n",
    "            [4, 128],\n",
    "            [5, 256],\n",
    "            [6, 512],\n",
    "            [7, 512]\n",
    "        ],\n",
    "        'activation': 'tanh',\n",
    "        # for the character embedding\n",
    "        'embedding': {'dim': 16}\n",
    "        # for highway layers\n",
    "        # if omitted, then no highway layers\n",
    "        'n_highway': 2,\n",
    "        }\n",
    "        '''\n",
    "        projection_dim = self.options['lstm']['projection_dim']\n",
    "\n",
    "        cnn_options = self.options['char_cnn']\n",
    "        filters = cnn_options['filters']\n",
    "        n_filters = sum(f[1] for f in filters)\n",
    "        max_chars = cnn_options['max_characters_per_token']\n",
    "        char_embed_dim = cnn_options['embedding']['dim']\n",
    "        n_chars = cnn_options['n_characters']\n",
    "        if n_chars != 262:\n",
    "            raise InvalidNumberOfCharacters(\n",
    "                \"Set n_characters=262 after training see the README.md\"\n",
    "            )\n",
    "        if cnn_options['activation'] == 'tanh':\n",
    "            activation = tf.nn.tanh\n",
    "        elif cnn_options['activation'] == 'relu':\n",
    "            activation = tf.nn.relu\n",
    "\n",
    "        # the character embeddings\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            self.embedding_weights = tf.get_variable(\n",
    "                    \"char_embed\", [n_chars, char_embed_dim],\n",
    "                    dtype=DTYPE,\n",
    "                    initializer=tf.random_uniform_initializer(-1.0, 1.0)\n",
    "            )\n",
    "            # shape (batch_size, unroll_steps, max_chars, embed_dim)\n",
    "            self.char_embedding = tf.nn.embedding_lookup(self.embedding_weights,\n",
    "                                                    self.ids_placeholder)\n",
    "\n",
    "        # the convolutions\n",
    "        def make_convolutions(inp):\n",
    "            with tf.variable_scope('CNN') as scope:\n",
    "                convolutions = []\n",
    "                for i, (width, num) in enumerate(filters):\n",
    "                    if cnn_options['activation'] == 'relu':\n",
    "                        # He initialization for ReLU activation\n",
    "                        # with char embeddings init between -1 and 1\n",
    "                        #w_init = tf.random_normal_initializer(\n",
    "                        #    mean=0.0,\n",
    "                        #    stddev=np.sqrt(2.0 / (width * char_embed_dim))\n",
    "                        #)\n",
    "\n",
    "                        # Kim et al 2015, +/- 0.05\n",
    "                        w_init = tf.random_uniform_initializer(\n",
    "                            minval=-0.05, maxval=0.05)\n",
    "                    elif cnn_options['activation'] == 'tanh':\n",
    "                        # glorot init\n",
    "                        w_init = tf.random_normal_initializer(\n",
    "                            mean=0.0,\n",
    "                            stddev=np.sqrt(1.0 / (width * char_embed_dim))\n",
    "                        )\n",
    "                    w = tf.get_variable(\n",
    "                        \"W_cnn_%s\" % i,\n",
    "                        [1, width, char_embed_dim, num],\n",
    "                        initializer=w_init,\n",
    "                        dtype=DTYPE)\n",
    "                    b = tf.get_variable(\n",
    "                        \"b_cnn_%s\" % i, [num], dtype=DTYPE,\n",
    "                        initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "                    conv = tf.nn.conv2d(\n",
    "                            inp, w,\n",
    "                            strides=[1, 1, 1, 1],\n",
    "                            padding=\"VALID\") + b\n",
    "                    # now max pool\n",
    "                    conv = tf.nn.max_pool(\n",
    "                            conv, [1, 1, max_chars-width+1, 1],\n",
    "                            [1, 1, 1, 1], 'VALID')\n",
    "\n",
    "                    # activation\n",
    "                    conv = activation(conv)\n",
    "                    conv = tf.squeeze(conv, squeeze_dims=[2])\n",
    "\n",
    "                    convolutions.append(conv)\n",
    "\n",
    "            return tf.concat(convolutions, 2)\n",
    "\n",
    "        embedding = make_convolutions(self.char_embedding)\n",
    "\n",
    "        # for highway and projection layers\n",
    "        n_highway = cnn_options.get('n_highway')\n",
    "        use_highway = n_highway is not None and n_highway > 0\n",
    "        use_proj = n_filters != projection_dim\n",
    "\n",
    "        if use_highway or use_proj:\n",
    "            #   reshape from (batch_size, n_tokens, dim) to (-1, dim)\n",
    "            batch_size_n_tokens = tf.shape(embedding)[0:2]\n",
    "            embedding = tf.reshape(embedding, [-1, n_filters])\n",
    "\n",
    "        # set up weights for projection\n",
    "        if use_proj:\n",
    "            assert n_filters > projection_dim\n",
    "            with tf.variable_scope('CNN_proj') as scope:\n",
    "                    W_proj_cnn = tf.get_variable(\n",
    "                        \"W_proj\", [n_filters, projection_dim],\n",
    "                        initializer=tf.random_normal_initializer(\n",
    "                            mean=0.0, stddev=np.sqrt(1.0 / n_filters)),\n",
    "                        dtype=DTYPE)\n",
    "                    b_proj_cnn = tf.get_variable(\n",
    "                        \"b_proj\", [projection_dim],\n",
    "                        initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=DTYPE)\n",
    "\n",
    "        # apply highways layers\n",
    "        def high(x, ww_carry, bb_carry, ww_tr, bb_tr):\n",
    "            carry_gate = tf.nn.sigmoid(tf.matmul(x, ww_carry) + bb_carry)\n",
    "            transform_gate = tf.nn.relu(tf.matmul(x, ww_tr) + bb_tr)\n",
    "            return carry_gate * transform_gate + (1.0 - carry_gate) * x\n",
    "\n",
    "        if use_highway:\n",
    "            highway_dim = n_filters\n",
    "\n",
    "            for i in range(n_highway):\n",
    "                with tf.variable_scope('CNN_high_%s' % i) as scope:\n",
    "                    W_carry = tf.get_variable(\n",
    "                        'W_carry', [highway_dim, highway_dim],\n",
    "                        # glorit init\n",
    "                        initializer=tf.random_normal_initializer(\n",
    "                            mean=0.0, stddev=np.sqrt(1.0 / highway_dim)),\n",
    "                        dtype=DTYPE)\n",
    "                    b_carry = tf.get_variable(\n",
    "                        'b_carry', [highway_dim],\n",
    "                        initializer=tf.constant_initializer(-2.0),\n",
    "                        dtype=DTYPE)\n",
    "                    W_transform = tf.get_variable(\n",
    "                        'W_transform', [highway_dim, highway_dim],\n",
    "                        initializer=tf.random_normal_initializer(\n",
    "                            mean=0.0, stddev=np.sqrt(1.0 / highway_dim)),\n",
    "                        dtype=DTYPE)\n",
    "                    b_transform = tf.get_variable(\n",
    "                        'b_transform', [highway_dim],\n",
    "                        initializer=tf.constant_initializer(0.0),\n",
    "                        dtype=DTYPE)\n",
    "\n",
    "                embedding = high(embedding, W_carry, b_carry,\n",
    "                                 W_transform, b_transform)\n",
    "\n",
    "        # finally project down if needed\n",
    "        if use_proj:\n",
    "            embedding = tf.matmul(embedding, W_proj_cnn) + b_proj_cnn\n",
    "\n",
    "        # reshape back to (batch_size, tokens, dim)\n",
    "        if use_highway or use_proj:\n",
    "            shp = tf.concat([batch_size_n_tokens, [projection_dim]], axis=0)\n",
    "            embedding = tf.reshape(embedding, shp)\n",
    "\n",
    "        # at last assign attributes for remainder of the model\n",
    "        self.embedding = embedding\n",
    "\n",
    "\n",
    "    def _build_word_embeddings(self):\n",
    "        projection_dim = self.options['lstm']['projection_dim']\n",
    "\n",
    "        # the word embeddings\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            self.embedding_weights = tf.get_variable(\n",
    "                \"embedding\", [self._n_tokens_vocab, projection_dim],\n",
    "                dtype=DTYPE,\n",
    "            )\n",
    "            self.embedding = tf.nn.embedding_lookup(self.embedding_weights,\n",
    "                                                self.ids_placeholder)\n",
    "\n",
    "\n",
    "    def _build_lstms(self):\n",
    "        # now the LSTMs\n",
    "        # these will collect the initial states for the forward\n",
    "        #   (and reverse LSTMs if we are doing bidirectional)\n",
    "\n",
    "        # parse the options\n",
    "        lstm_dim = self.options['lstm']['dim']\n",
    "        projection_dim = self.options['lstm']['projection_dim']\n",
    "        n_lstm_layers = self.options['lstm'].get('n_layers', 1)\n",
    "        cell_clip = self.options['lstm'].get('cell_clip')\n",
    "        proj_clip = self.options['lstm'].get('proj_clip')\n",
    "        use_skip_connections = self.options['lstm']['use_skip_connections']\n",
    "        if use_skip_connections:\n",
    "            print(\"USING SKIP CONNECTIONS\")\n",
    "        else:\n",
    "            print(\"NOT USING SKIP CONNECTIONS\")\n",
    "\n",
    "        # the sequence lengths from input mask\n",
    "        if self.use_character_inputs:\n",
    "            mask = tf.reduce_any(self.ids_placeholder > 0, axis=2)\n",
    "        else:\n",
    "            mask = self.ids_placeholder > 0\n",
    "        sequence_lengths = tf.reduce_sum(tf.cast(mask, tf.int32), axis=1)\n",
    "        batch_size = tf.shape(sequence_lengths)[0]\n",
    "\n",
    "        # for each direction, we'll store tensors for each layer\n",
    "        self.lstm_outputs = {'forward': [], 'backward': []}\n",
    "        self.lstm_state_sizes = {'forward': [], 'backward': []}\n",
    "        self.lstm_init_states = {'forward': [], 'backward': []}\n",
    "        self.lstm_final_states = {'forward': [], 'backward': []}\n",
    "\n",
    "        update_ops = []\n",
    "        for direction in ['forward', 'backward']:\n",
    "            if direction == 'forward':\n",
    "                layer_input = self.embedding\n",
    "            else:\n",
    "                layer_input = tf.reverse_sequence(\n",
    "                    self.embedding,\n",
    "                    sequence_lengths,\n",
    "                    seq_axis=1,\n",
    "                    batch_axis=0\n",
    "                )\n",
    "\n",
    "            for i in range(n_lstm_layers):\n",
    "                if projection_dim < lstm_dim:\n",
    "                    # are projecting down output\n",
    "                    lstm_cell = tf.nn.rnn_cell.LSTMCell(\n",
    "                        lstm_dim, num_proj=projection_dim,\n",
    "                        cell_clip=cell_clip, proj_clip=proj_clip)\n",
    "                else:\n",
    "                    lstm_cell = tf.nn.rnn_cell.LSTMCell(\n",
    "                            lstm_dim,\n",
    "                            cell_clip=cell_clip, proj_clip=proj_clip)\n",
    "\n",
    "                if use_skip_connections:\n",
    "                    # ResidualWrapper adds inputs to outputs\n",
    "                    if i == 0:\n",
    "                        # don't add skip connection from token embedding to\n",
    "                        # 1st layer output\n",
    "                        pass\n",
    "                    else:\n",
    "                        # add a skip connection\n",
    "                        lstm_cell = tf.nn.rnn_cell.ResidualWrapper(lstm_cell)\n",
    "\n",
    "                # collect the input state, run the dynamic rnn, collect\n",
    "                # the output\n",
    "                state_size = lstm_cell.state_size\n",
    "                # the LSTMs are stateful.  To support multiple batch sizes,\n",
    "                # we'll allocate size for states up to max_batch_size,\n",
    "                # then use the first batch_size entries for each batch\n",
    "                init_states = [\n",
    "                    tf.Variable(\n",
    "                        tf.zeros([self._max_batch_size, dim]),\n",
    "                        trainable=False\n",
    "                    )\n",
    "                    for dim in lstm_cell.state_size\n",
    "                ]\n",
    "                batch_init_states = [\n",
    "                    state[:batch_size, :] for state in init_states\n",
    "                ]\n",
    "\n",
    "                if direction == 'forward':\n",
    "                    i_direction = 0\n",
    "                else:\n",
    "                    i_direction = 1\n",
    "                variable_scope_name = 'RNN_{0}/RNN/MultiRNNCell/Cell{1}'.format(\n",
    "                    i_direction, i)\n",
    "                with tf.variable_scope(variable_scope_name):\n",
    "                    layer_output, final_state = tf.nn.dynamic_rnn(\n",
    "                        lstm_cell,\n",
    "                        layer_input,\n",
    "                        sequence_length=sequence_lengths,\n",
    "                        initial_state=tf.nn.rnn_cell.LSTMStateTuple(\n",
    "                            *batch_init_states),\n",
    "                    )\n",
    "\n",
    "                self.lstm_state_sizes[direction].append(lstm_cell.state_size)\n",
    "                self.lstm_init_states[direction].append(init_states)\n",
    "                self.lstm_final_states[direction].append(final_state)\n",
    "                if direction == 'forward':\n",
    "                    self.lstm_outputs[direction].append(layer_output)\n",
    "                else:\n",
    "                    self.lstm_outputs[direction].append(\n",
    "                        tf.reverse_sequence(\n",
    "                            layer_output,\n",
    "                            sequence_lengths,\n",
    "                            seq_axis=1,\n",
    "                            batch_axis=0\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                with tf.control_dependencies([layer_output]):\n",
    "                    # update the initial states\n",
    "                    for i in range(2):\n",
    "                        new_state = tf.concat(\n",
    "                            [final_state[i][:batch_size, :],\n",
    "                             init_states[i][batch_size:, :]], axis=0)\n",
    "                        state_update_op = tf.assign(init_states[i], new_state)\n",
    "                        update_ops.append(state_update_op)\n",
    "    \n",
    "                layer_input = layer_output\n",
    "\n",
    "        self.mask = mask\n",
    "        self.sequence_lengths = sequence_lengths\n",
    "        self.update_state_op = tf.group(*update_ops)\n",
    "\n",
    "\n",
    "def dump_token_embeddings(vocab_file, options_file, weight_file, outfile):\n",
    "    '''\n",
    "    Given an input vocabulary file, dump all the token embeddings to the\n",
    "    outfile.  The result can be used as the embedding_weight_file when\n",
    "    constructing a BidirectionalLanguageModel.\n",
    "    '''\n",
    "    with open(options_file, 'r') as fin:\n",
    "        options = json.load(fin)\n",
    "    max_word_length = options['char_cnn']['max_characters_per_token']\n",
    "\n",
    "    vocab = UnicodeCharsVocabulary(vocab_file, max_word_length)\n",
    "    batcher = Batcher(vocab_file, max_word_length)\n",
    "\n",
    "    ids_placeholder = tf.placeholder('int32',\n",
    "                                     shape=(None, None, max_word_length)\n",
    "    )\n",
    "    model = BidirectionalLanguageModel(options_file, weight_file)\n",
    "    embedding_op = model(ids_placeholder)['token_embeddings']\n",
    "\n",
    "    n_tokens = vocab.size\n",
    "    embed_dim = int(embedding_op.shape[2])\n",
    "\n",
    "    embeddings = np.zeros((n_tokens, embed_dim), dtype=DTYPE)\n",
    "\n",
    "    config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for k in range(n_tokens):\n",
    "            token = vocab.id_to_word(k)\n",
    "            char_ids = batcher.batch_sentences([[token]])[0, 1, :].reshape(\n",
    "                1, 1, -1)\n",
    "            embeddings[k, :] = sess.run(\n",
    "                embedding_op, feed_dict={ids_placeholder: char_ids}\n",
    "            )\n",
    "\n",
    "    with h5py.File(outfile, 'w') as fout:\n",
    "        ds = fout.create_dataset(\n",
    "            'embedding', embeddings.shape, dtype='float32', data=embeddings\n",
    "        )\n",
    "\n",
    "def dump_bilm_embeddings(vocab_file, dataset_file, options_file,\n",
    "                         weight_file, outfile):\n",
    "    with open(options_file, 'r') as fin:\n",
    "        options = json.load(fin)\n",
    "    max_word_length = options['char_cnn']['max_characters_per_token']\n",
    "\n",
    "    vocab = UnicodeCharsVocabulary(vocab_file, max_word_length)\n",
    "    batcher = Batcher(vocab_file, max_word_length)\n",
    "\n",
    "    ids_placeholder = tf.placeholder('int32',\n",
    "                                     shape=(None, None, max_word_length)\n",
    "    )\n",
    "    model = BidirectionalLanguageModel(options_file, weight_file)\n",
    "    ops = model(ids_placeholder)\n",
    "\n",
    "    config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sentence_id = 0\n",
    "        with open(dataset_file, 'r') as fin, h5py.File(outfile, 'w') as fout:\n",
    "            for line in fin:\n",
    "                sentence = line.strip().split()\n",
    "                char_ids = batcher.batch_sentences([sentence])\n",
    "                embeddings = sess.run(\n",
    "                    ops['lm_embeddings'], feed_dict={ids_placeholder: char_ids}\n",
    "                )\n",
    "                ds = fout.create_dataset(\n",
    "                    '{}'.format(sentence_id),\n",
    "                    embeddings.shape[1:], dtype='float32',\n",
    "                    data=embeddings[0, :, :, :]\n",
    "                )\n",
    "\n",
    "                sentence_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-court",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
