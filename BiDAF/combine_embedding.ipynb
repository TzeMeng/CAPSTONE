{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "source": [
    "### Load BERT Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                             context  contextID\n",
       "0  North Carolina consists of three main geograph...          1\n",
       "1  The coastal plain transitions to the Piedmont ...          2\n",
       "2  The western section of the state is part of th...          3\n",
       "3  The climate of the coastal plain is influenced...          4\n",
       "4  The Atlantic Ocean has less influence on the c...          5"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>context</th>\n      <th>contextID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>North Carolina consists of three main geograph...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The coastal plain transitions to the Piedmont ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The western section of the state is part of th...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The climate of the coastal plain is influenced...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The Atlantic Ocean has less influence on the c...</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "train_context = pd.read_excel('./data/train/train_context.xlsx')\n",
    "train_context.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"The coastal plain transitions to the Piedmont region along the Atlantic Seaboard fall line, a line which marks the elevation at which waterfalls first appear on streams and rivers. The Piedmont region of central North Carolina is the state's most urbanized and densely populated section. It consists of gently rolling countryside frequently broken by hills or low mountain ridges. Small, isolated, and deeply eroded mountain ranges and peaks are located in the Piedmont, including the Sauratown Mountains, Pilot Mountain, the Uwharrie Mountains, Crowder's Mountain, King's Pinnacle, the Brushy Mountains, and the South Mountains. The Piedmont ranges from about 300 to 400 feet (91 to 122 m) in elevation in the east to over 1,000 feet (300 m) in the west. Because of the rapid population growth in the Piedmont, a significant part of the rural area in this region is being transformed into suburbs with shopping centers, housing, and corporate offices. Agriculture is steadily declining in importance. The major rivers of the Piedmont, such as the Yadkin and Catawba, tend to be fast-flowing, shallow, and narrow.\""
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "context_para = train_context['context']\n",
    "context_para[1]"
   ]
  },
  {
   "source": [
    "### Train and save BERT embeddings done on SQuAD \"Train context\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_size = 100\n",
    "\n",
    "try:\n",
    "    bert_embeddings = pd.read_pickle('./context/content_embed_{}.pkl'.format(embedding_size))\n",
    "\n",
    "except:\n",
    "    ### Build embeddding\n",
    "    bert_embeddings = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tf.device('/gpu:1'):\n",
    "            for para in tqdm(context_para):\n",
    "                words = para.split()\n",
    "                print()\n",
    "                for word in words:\n",
    "                    if word in bert_embeddings.keys():\n",
    "                        continue\n",
    "                    else:\n",
    "                        input_ids = tf.constant(tokenizer.encode(word))[None, :]  # Batch size 1\n",
    "                        outputs = model(input_ids)\n",
    "                        last_hidden_states = outputs[-1]  # The last hidden-state is the first element of the output tuple\n",
    "                        bert_embeddings[word] = last_hidden_states[0][:embedding_size]\n",
    "                        \n",
    "    ## Saving BERT Embeddings\n",
    "    with open(\"./context/content_embed_{}.pkl\".format(embedding_size), \"wb\") as e:\n",
    "        pickle.dump(bert_embeddings, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tf.constant(tokenizer.encode(\"ember is bad\"))[None, :]  # Batch size 1\n",
    "outputs = model(input_ids)\n",
    "last_hidden_states = outputs[-1]  # The last hidden-state is the first element of the output tuple\n",
    "hazel = last_hidden_states[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidaf_embedding= pd.read_pickle('./data/train/no_random_word_embeddings.pkl')\n",
    "bidaf_word2idx = pd.read_pickle('./data/train/word2idx.pkl')"
   ]
  },
  {
   "source": [
    "### Merge Embedding\n",
    "outOfVocab : Refers to words that are not found in the GloVe corpus, thus were randomly intialized in the original BiDAF embeddings, however this words were found and replaced by the embeddings done in BERT <br>\n",
    "\n",
    "not_found: This refers to words not found in both the BiDAF and BERT embeddings, thus we had to revert back to random initialised vectors for this words. From further inspections these are words that are misspelled <br>\n",
    "\n",
    "matched : This refers to words that appeared in both the BiDAF as well as the BERT embeddings, thus we just concatanate the embeddings<br>\n",
    "\n",
    "bidaf_ed : This refers to words only found in the bidaf model thus to ensure that the word embedding tensor is of the same size, we duplicate the tensor and concatanate it with itself\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
      "                                                                                                                                       \n",
      "repiprocity\n",
      "elctrical\n",
      "deide\n",
      "nedded\n",
      "imprtant\n",
      "curren\n",
      "originial\n",
      "beng\n",
      "inductibe\n",
      "sturdier\n",
      "tricky\n",
      "pictoral\n",
      "useable\n",
      "iclude\n",
      "Wats\n",
      "imagenary\n",
      "perpindicular\n",
      "empedance\n",
      "transmmiting\n",
      "undirectional\n",
      "ral\n",
      "m/2\n",
      "lengthed\n",
      "rection\n",
      "adressed\n",
      "structuak\n",
      "stron\n",
      "asympotic\n",
      "indiviual\n",
      "mathmatic\n",
      "programmin\n",
      "beahvior\n",
      "transmtting\n",
      "equivalency\n",
      "Upright\n",
      "meisporangia\n",
      "bixsexual\n",
      "anthophyta\n",
      "Eudicots\n",
      "Basal\n",
      "enveloping\n",
      "heteromorphic\n",
      "homomorphic\n",
      "Chaminar\n",
      "Calukya\n",
      "Behmani\n",
      "wellness\n",
      "langauge\n",
      "minarets\n",
      "Talangana\n",
      "stall\n",
      "peformance\n",
      "20015\n",
      "G.M.C.\n",
      "Balayohi\n",
      "Planes\n",
      "Sanat\n",
      "buildt\n",
      "famer\n",
      "Washing\n",
      "Designintellence\n",
      "NH\n",
      "Blocs\n",
      "BLOCS\n",
      "vouched\n",
      "Excitement\n",
      "Stimulating\n",
      "Crushing\n",
      "Vibrating\n",
      "amputee\n",
      "opiods\n",
      "pontificating\n",
      "Melack\n",
      "sheaths\n",
      "newsborns\n",
      "measurably\n",
      "treament\n",
      "heirarchical\n",
      "accellerator\n",
      "Asking\n",
      "requiret\n",
      "U.S.-\n",
      "Stiwell\n",
      "E.University\n",
      "reincorperated\n",
      "whitewashing\n",
      "motorist\n",
      "occure\n",
      "Areminia\n",
      "invadable\n",
      "669\n",
      "vanquished\n",
      "jalalians\n",
      "Abdu'l\n",
      "Dashnak\n",
      "ratifies\n",
      "uban\n",
      "hamaynker\n",
      "Arnmenian\n",
      "IFI\n",
      "YMSU\n",
      "competiting\n",
      "cyclying\n",
      "similair\n",
      "lenght\n",
      "milliltre\n",
      "Stromatolites\n",
      "Neiserria\n",
      "biofilm\n",
      "nutriment\n",
      "membrabe\n",
      "Absence\n",
      "intracellar\n",
      "respoonsible\n",
      "criterias\n",
      "eelectron\n",
      "repiratory\n",
      "photorophy\n",
      "preffered\n",
      "indentify\n",
      "statge\n",
      "burgdoferi\n",
      "E.coli\n",
      "Grahm\n",
      "Norcadia\n",
      "disiease\n",
      "bacterocidal\n",
      "cyling\n",
      "aknowledged\n",
      "Enrlich\n",
      "depanelization\n",
      "Etch\n",
      "unpopulated\n",
      "epoxies\n",
      "shiny\n",
      "nested\n",
      "ethnocentric\n",
      "Pietho\n",
      "gong\n",
      "scuttle\n",
      "mainstreams\n",
      "exsistence\n",
      "appreciations\n",
      "Rupublic\n",
      "isle\n",
      "significances\n",
      "Whee\n",
      "modelers\n",
      "Greecs\n",
      "relevancy\n",
      "Jacquelyn\n",
      "efficients\n",
      "threateded\n",
      "blubs\n",
      "critcism\n",
      "excutive\n",
      "Premeir\n",
      "qualifty\n",
      "champtions\n",
      "bumped\n",
      "Leaque\n",
      "teleivsion\n",
      "unlicenced\n",
      "regardin\n",
      "OFfice\n",
      "licenced\n",
      "citie\n",
      "chieftan\n",
      "overburdened\n",
      "withdrawl\n",
      "implented\n",
      "Fourt\n",
      "usurp\n",
      "Sull\n",
      "Mithridate\n",
      "consulars\n",
      "abillity\n",
      "vacating\n",
      "satiated\n",
      "instigator\n",
      "exuberance\n",
      "counteracted\n",
      "horseman\n",
      "entireity\n",
      "harbinger\n",
      "striping\n",
      "womans\n",
      "mans\n",
      "defame\n",
      "poltical\n",
      "musics\n",
      "Maritus\n",
      "adobt\n",
      "costal\n",
      "conscript\n",
      "obsticles\n",
      "disuade\n",
      "defeate\n",
      "ove\n",
      "btween\n",
      "aattack\n",
      "Southheast\n",
      "Washingto\n",
      "Yamaoto\n",
      "bobers\n",
      "attck\n",
      "releived\n",
      "repeatedly?forces\n",
      "folloing\n",
      "Austrailia\n",
      "Nimitze\n",
      "aling\n",
      "aircrews\n",
      "namee\n",
      "Kurit\n",
      "Kincaid\n",
      "Japanes\n",
      "reinforcments\n",
      "thhey\n",
      "B=17\n",
      "incendry\n",
      "forst\n",
      "nuked\n",
      "resum\n",
      "delagation\n",
      "Junipero\n",
      "Mexcians\n",
      "urgan\n",
      "15.9.%\n",
      "millenials\n",
      "familes\n",
      "volumte\n",
      "inveted\n",
      "Zucche\n",
      "beging\n",
      "theri\n",
      "Internatonal\n",
      "westerner\n",
      "chracter\n",
      "Hiljri\n",
      "Calender\n",
      "4395\n",
      "Millenium\n",
      "Irea\n",
      "334BC\n",
      "600s\n",
      "nonconverted\n",
      "Islamicized\n",
      "migranted\n",
      "Turkics\n",
      "Tumrids\n",
      "neighbored\n",
      "ALl\n",
      "Capsian\n",
      "northen\n",
      "29c\n",
      "Iranina\n",
      "exective\n",
      "noncompetitive\n",
      "COmpetitiveness\n",
      "Teheran\n",
      "Bahai\n",
      "Archaemenids\n",
      "Answer\n",
      "Achaemind\n",
      "conqured\n",
      "highth\n",
      "begain\n",
      "tress\n",
      "abundent\n",
      "nake\n",
      "Cetic\n",
      "122AD\n",
      "governements\n",
      "Unionists\n",
      "Irelandand\n",
      "popoular\n",
      "purposed\n",
      "chuck\n",
      "wogabaliri\n",
      "Organizes\n",
      "C.W.\n",
      "wron\n",
      "goalpost\n",
      "Hammon\n",
      "Georgain\n",
      "tubman\n",
      "coupe\n",
      "tubmans\n",
      "liberian\n",
      "vai\n",
      "ellen\n",
      "sirleaf\n",
      "Monrova\n",
      "hypobolic\n",
      "quanternions\n",
      "G.B.\n",
      "Princpia\n",
      "Gödels\n",
      "metaphysician\n",
      "Assumptions\n",
      "reimagine\n",
      "whitehead\n",
      "Concepts\n",
      "Prehensio\n",
      "philosphers\n",
      "Harthorne\n",
      "Whiteheads\n",
      "Jeanine\n",
      "overused\n",
      "revolutinzed\n",
      "Photodermatitis\n",
      "Whhat\n",
      "bever\n",
      "activitty\n",
      "sprectrum\n",
      "2000's/\n",
      "anitibiotics\n",
      "semisytetic\n",
      "penicilin\n",
      "slufonamides\n",
      "Delbruck\n",
      "hje\n",
      "occurs/\n",
      "antimicrobials\n",
      "pipelin\n",
      "jupiter\n",
      "Availablity\n",
      "personable\n",
      "comman\n",
      "dalog\n",
      "Exploer\n",
      "airplone\n",
      "privilige\n",
      "Chromes\n",
      "accesible\n",
      "evironment\n",
      "resoultion\n",
      "uprgrade\n",
      "certificiation\n",
      "windos\n",
      "2013/\n",
      "Androids\n",
      "Vistas\n",
      "WIndows\n",
      "signular\n",
      "portioning\n",
      "crownd\n",
      "disputre\n",
      "hegenomy\n",
      "Duties\n",
      "prevelant\n",
      "Limkoking\n",
      "vertaling\n",
      "divvied\n",
      "Unedited\n",
      "Disparities\n",
      "aerodome\n",
      "glidescope\n",
      "assures\n",
      "Keivan\n",
      "Limen\n",
      "Tripe\n",
      "accourding\n",
      "beleive\n",
      "againts\n",
      "rulled\n",
      "Novgotod\n",
      "Whoch\n",
      "teritories\n",
      "Wat\n",
      "Ruik\n",
      "citis\n",
      "enimies\n",
      "teritory\n",
      "Mehodius\n",
      "oleg\n",
      "Constantinole\n",
      "prophesised\n",
      "Igot\n",
      "drivin\n",
      "Byzatines\n",
      "Vladimar\n",
      "terriroty\n",
      "terrirory\n",
      "relatinship\n",
      "mongol\n",
      "patriachic\n",
      "Yarolav\n",
      "countried\n",
      "frequesntly\n",
      "Whichgroup\n",
      "Pecheneges\n",
      "uncertian\n",
      "Titches\n",
      "yen\n",
      "SNESes\n",
      "Sumerican\n",
      "gasp\n",
      "wan\n",
      "harrow\n",
      "rake\n",
      "Mendana\n",
      "isalnds\n",
      "envirnmental\n",
      "islander\n",
      "ano\n",
      "losefa\n",
      "Seaman'd\n",
      "aprt\n",
      "Ninos\n",
      "arguable\n",
      "impregnation\n",
      "savior\n",
      "wayward\n",
      "Sanctified\n",
      "sinner\n",
      "practce\n",
      "volumed\n",
      "plagiarism\n",
      "twinkles\n",
      "implantation\n",
      "Urinating\n",
      "footrace\n",
      "wothwhile\n",
      "Supports\n",
      "disaters\n",
      "causalities\n",
      "NAFF\n",
      "ACEE\n",
      "airman\n",
      "retests\n",
      "Mosley\n",
      "dismantlement\n",
      "trichromatic\n",
      "NUVs\n",
      "will%\n",
      "rgb\n",
      "everday\n",
      "opto\n",
      "orignal\n",
      "vienna\n",
      "versailles\n",
      "organiztion\n",
      "ecomonies\n",
      "Dromaeosaurids\n",
      "Pectoralis\n",
      "shoebirds\n",
      "Alloparenting\n",
      "particulary\n",
      "recesive\n",
      "Manchun\n",
      "ming\n",
      "originall\n",
      "Manchuse\n",
      "ethnice\n",
      "Shunzi\n",
      "Liandong\n",
      "Kanxi\n",
      "behead\n",
      "persecutions\n",
      "Qianlongs\n",
      "redo\n",
      "Whed\n",
      "emblam\n",
      "socity\n",
      "Mings\n",
      "poety\n",
      "Suiyaun\n",
      "cit\n",
      "domesticate\n",
      "Squash\n",
      "Convetion\n",
      "Brazillians\n",
      "intermarrying\n",
      "Pipll\n",
      "Mayanga\n",
      "amoun\n",
      "bural\n",
      "Aermenian\n",
      "vulgate\n",
      "Cinnabar\n",
      "vincent\n",
      "Placing\n",
      "penetate\n",
      "mandrill\n",
      "circimstance\n",
      "movment\n",
      "monumuents\n",
      "popultaion\n",
      "hieroglyghics\n",
      "upheval\n",
      "maked\n",
      "Kindom\n",
      "Prolemaic\n",
      "buld\n",
      "polidy\n",
      "protazoa\n",
      "tems\n",
      "responders\n",
      "ppopulation\n",
      "computerised\n",
      "NAgi\n",
      "Shatata\n",
      "egyptians\n",
      "reulted\n",
      "Forign\n",
      "exhange\n",
      "compnaies\n",
      "radiply\n",
      "serenity\n",
      "sonsumers\n",
      "consideredt\n",
      "waer\n",
      "rufugees\n",
      "archaelogical\n",
      "laguages\n",
      "Hmadi\n",
      "syles\n",
      "Munammad\n",
      "Huayn\n",
      "polular\n",
      "Guiness\n",
      "Deepest\n",
      "kingdomwas\n",
      "Tiyns\n",
      "caesar\n",
      "Baptistry\n",
      "katharos\n",
      "Sobina\n",
      "ROme\n",
      "lateran\n",
      "1106\n",
      "maria\n",
      "medallion\n",
      "navicella\n",
      "sicily\n",
      "greeks\n",
      "redecorate\n",
      "raphael\n",
      "paneling\n",
      "tessere\n",
      "robotically\n",
      "amde\n",
      "Universitas\n",
      "Madrasas\n",
      "shortens\n",
      "Melancthon\n",
      "Resisting\n",
      "personify\n",
      "Gladiatorial\n",
      "provencial\n",
      "augers\n",
      "predictors\n",
      "expiate\n",
      "lapped\n",
      "legionnaires\n",
      "potions\n",
      "godhood\n",
      "lifeime\n",
      "notiable\n",
      "cultism\n",
      "Maximillian\n",
      "admonish\n",
      "pontifax\n",
      "YOutube\n",
      "Smartphone\n",
      "streamable\n",
      "revolutionize\n",
      "kony\n",
      "impersonate\n",
      "ling\n",
      "Posting\n",
      "google\n",
      "contractees\n",
      "Issac\n",
      "reiterate\n",
      "reimbursed\n",
      "COurt\n",
      "perks\n",
      "Separationists\n",
      "supremecy\n",
      "Waldensian\n",
      "theses/\n",
      "upswing\n",
      "dbate\n",
      "snate\n",
      "Delay\n",
      "bdcome\n",
      "Gephart\n",
      "seconde\n",
      "despcription\n",
      "funcioning\n",
      "Dmocratic\n",
      "gloof\n",
      "1876,77,79\n",
      "Rober\n",
      "prominenct\n",
      "Obligations\n",
      "Obstruction\n",
      "pertition\n",
      "consitent\n",
      "committ\n",
      "ornately\n",
      "9th-11th\n",
      "movmeent\n",
      "14:1\n",
      "WTS\n",
      "clarifies\n",
      "errs\n",
      "smugly\n",
      "Mennonite\n",
      "brevetted\n",
      "quip\n",
      "federalize\n",
      "subpoena\n",
      "BID\n",
      "nonreligious\n",
      "Prepatory\n",
      "Recover\n",
      "Overvaluation\n",
      "CDOS\n",
      "entices\n",
      "Timoty\n",
      "volatilaty\n",
      "erronesously\n",
      "Bjornhold\n",
      "4Q\n",
      "1Q\n",
      "U.E.\n",
      "Brazel\n",
      "Conimbriga\n",
      "Codoba\n",
      "Lanugage\n",
      "Tavora\n",
      "Pambal\n",
      "Geres\n",
      "Seguranca\n",
      "Publica\n",
      "Judiciaria\n",
      "decriminalize\n",
      "administrates\n",
      "Libon\n",
      "Ciencia\n",
      "Agucadoura\n",
      "Tomas\n",
      "garnished\n",
      "adopter\n",
      "Pleto\n",
      "wotked\n",
      "utilitatian\n",
      "conetemporary\n",
      "naysayer\n",
      "Sibera\n",
      "Tehtys\n",
      "aboutt\n",
      "sediements\n",
      "oregenies\n",
      "Glaciars\n",
      "Mounties\n",
      "Pinkertons\n",
      "Hornigk\n",
      "IGO\n",
      "unaccountable\n",
      "motorcyclist\n",
      "atrocity\n",
      "constricted\n",
      "Bjornson\n",
      "Signatories\n",
      "CPPC\n",
      "Perpetrators\n",
      "agains\n",
      "Convicted\n",
      "Popovic\n",
      "Nikolic\n",
      "convicting\n",
      "seasoning\n",
      "propoerty\n",
      "beggining\n",
      "Item\n",
      "surgeonfish\n",
      "culutres\n",
      "kindoms\n",
      "CHinese\n",
      "regin\n",
      "Ymayyads\n",
      "contorl\n",
      "Tansaxania\n",
      "Trukestan\n",
      "happeded\n",
      "eachother\n",
      "boarder\n",
      "solider\n",
      "mountian\n",
      "thrid\n",
      "maining\n",
      "mountians\n",
      "impliment\n",
      "Cavanugh\n",
      "annuals\n",
      "Nore\n",
      "aeronatical\n",
      "tot\n",
      "Malinoswki\n",
      "needlessly\n",
      "Ethnoarchaeologists\n",
      "inclusively\n",
      "folkore\n",
      "anthrozoology\n",
      "Mapping\n",
      "Inquires\n",
      "popularion\n",
      "1002\n",
      "ABout\n",
      "Monatanas\n",
      "Wala\n",
      "congregant\n",
      "Richmind\n",
      "Interplay\n",
      "multcellular\n",
      "Geimsa\n",
      "enymzes\n",
      "specifc\n",
      "illnes\n",
      "Hunted\n",
      "Confrontational\n",
      "disrespecting\n",
      "regals\n",
      "reguarded\n",
      "phrased\n",
      "Tabago\n",
      "accommodations\n",
      "resently\n",
      "Jhocchen\n",
      "Durban\n",
      "evangelize\n",
      "Mudhum\n"
     ]
    }
   ],
   "source": [
    "outOfVocab = 0\n",
    "not_found = 0\n",
    "matched = 0\n",
    "bidaf_ed = 0\n",
    "\n",
    "words_not_found = []\n",
    "for key in bidaf_word2idx:\n",
    "    index = bidaf_word2idx[key]\n",
    "\n",
    "    if np.all(bidaf_embedding[index] == 1):\n",
    "        try:\n",
    "            bidaf_embedding[index] = np.concatenate([bert_embeddings[key],bert_embeddings[key]])\n",
    "            outOfVocab += 1\n",
    "        except:\n",
    "            bidaf_embedding[index] = np.random.normal(0, 0.1, 200)\n",
    "            print(key)\n",
    "            not_found += 1\n",
    "            words_not_found.append(key)\n",
    "    else:\n",
    "        try:\n",
    "            bidaf_embedding[index] = np.concatenate([bidaf_embedding[index],bert_embeddings[key]])\n",
    "            matched += 1\n",
    "\n",
    "        except:\n",
    "            bidaf_embedding[index] = np.concatenate([bidaf_embedding[index],bidaf_embedding[index]])\n",
    "            bidaf_ed += 1\n",
    "            print(key)\n",
    "            words_not_found.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "33518\n25960\n32361\n10782\n"
     ]
    }
   ],
   "source": [
    "print(outOfVocab)\n",
    "print(not_found)\n",
    "print(matched)\n",
    "print(bidaf_ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the embedding\n",
    "with open(\"./data/train/combined_word_embeddings.pkl\", \"wb\") as e:\n",
    "    pickle.dump(bidaf_embedding, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save words that cant be found in the BERT Embedding\n",
    "with open(\"./data/train/words_not_found.pkl\", \"wb\") as e:\n",
    "    pickle.dump(words_not_found, e)"
   ]
  },
  {
   "source": [
    "## Experiment with adding ELMO embedding since BERT did not have much help\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution() ## Disable eager execution for graph creation\n",
    "\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a random sentence\n",
    "x = [\"Roasted\"]\n",
    "\n",
    "def elmo_vectors(x):\n",
    "  embeddings = elmo(x, signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    # return average of ELMo features\n",
    "    return sess.run(tf.reduce_mean(embeddings,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "I0407 01:58:24.775568 16516 saver.py:1511] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 0.11338134, -0.22380908,  0.20500736,  0.06183614,  0.41078082,\n",
       "       -0.2128304 , -0.15933694,  0.40490472,  0.13175496,  0.31573987],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "y = elmo_vectors(x)\n",
    "y[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/14798 [00:00<?, ?it/s]I0407 01:53:28.033949 23384 saver.py:1511] Saver not created because there are no variables in the graph to restore\n",
      "  0%|          | 0/14798 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "InvalidArgumentError",
     "evalue": "Cannot colocate nodes node module_1_apply_default_14/map/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3 (defined at C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow_hub\\native_module.py:615) placed on device No device assignments were active during op 'module_1_apply_default_14/map/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3' creation.  and node module_1_apply_default_14/Reshape (defined at C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow_hub\\native_module.py:615) placed on device Device assignments active during op 'module_1_apply_default_14/Reshape' creation:\n  with tf.device(/device:CPU:0): <C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py:80>\n  with tf.device(/gpu:1): <<ipython-input-19-88410196d8f3>:10> : Cannot merge devices with incompatible types: '/device:GPU:1' and '/device:CPU:0'\n\t [[node module_1_apply_default_14/map/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3 (defined at C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow_hub\\native_module.py:615) ]]Additional information about colocations:No node-device colocations were active during op 'module_1_apply_default_14/map/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3' creation.\nNo device assignments were active during op 'module_1_apply_default_14/map/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3' creation.\nNo node-device colocations were active during op 'module_1_apply_default_14/Reshape' creation.\nDevice assignments active during op 'module_1_apply_default_14/Reshape' creation:\n  with tf.device(/device:CPU:0): <C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py:80>\n  with tf.device(/gpu:1): <<ipython-input-19-88410196d8f3>:10>\n\nOriginal stack trace for 'module_1_apply_default_14/map/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3':\n  File \"c:\\Users\\ngtze\\.vscode\\extensions\\ms-toolsai.jupyter-2021.5.702919634\\pythonFiles\\vscode_datascience_helpers\\..\\pyvsc-run-isolated.py\", line 30, in <module>\n    runpy.run_path(module, run_name=\"__main__\")\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\runpy.py\", line 265, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"c:\\Users\\ngtze\\.vscode\\extensions\\ms-toolsai.jupyter-2021.5.702919634\\pythonFiles\\vscode_datascience_helpers\\kernel_prewarm_starter.py\", line 31, in <module>\n    runpy.run_module(module, run_name=\"__main__\", alter_sys=False)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\runpy.py\", line 210, in run_module\n    return _run_code(code, {}, init_globals, run_name, mod_spec)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n    app.start()\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 149, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n    handle._run()\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\asyncio\\events.py\", line 81, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n    self.run()\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 543, in execute_request\n    self.do_execute(\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2877, in run_cell\n    result = self._run_cell(\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2923, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3146, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3418, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-19-88410196d8f3>\", line 17, in <module>\n    temp = elmo_vectors(x)\n  File \"<ipython-input-14-35d5d78b2e58>\", line 5, in elmo_vectors\n    embeddings = elmo(x, signature=\"default\", as_dict=True)[\"elmo\"]\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow_hub\\module.py\", line 266, in __call__\n    dict_outputs = self._impl.create_apply_graph(\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow_hub\\native_module.py\", line 615, in create_apply_graph\n    tf.compat.v1.train.import_meta_graph(\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1459, in import_meta_graph\n    return _import_meta_graph_with_return_elements(meta_graph_or_file,\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1480, in _import_meta_graph_with_return_elements\n    meta_graph.import_scoped_meta_graph_with_return_elements(\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\meta_graph.py\", line 799, in import_scoped_meta_graph_with_return_elements\n    imported_return_elements = importer.import_graph_def(\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 538, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py\", line 400, in import_graph_def\n    return _import_graph_def_internal(\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py\", line 513, in _import_graph_def_internal\n    _ProcessNewOps(graph)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py\", line 243, in _ProcessNewOps\n    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3678, in _add_new_tf_operations\n    new_ops = [\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3679, in <listcomp>\n    self._create_op_from_tf_operation(c_op, compute_device=compute_devices)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3561, in _create_op_from_tf_operation\n    ret = Operation(c_op, self)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1990, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-d1139ab28379>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0melmo_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./context/elmo_content_embed_{}.pkl'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression)\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[0;32m    498\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    500\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './context/elmo_content_embed_100.pkl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1374\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1375\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1376\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1358\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1359\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1397\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1398\u001b[1;33m       \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Cannot colocate nodes {{colocation_node module_1_apply_default_14/map/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3}} and {{colocation_node module_1_apply_default_14/Reshape}}: Cannot merge devices with incompatible types: '/device:GPU:1' and '/device:CPU:0'\n\t [[{{node module_1_apply_default_14/map/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-d1139ab28379>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                 \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melmo_vectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m                 \u001b[0melmo_embeddings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-35d5d78b2e58>\u001b[0m in \u001b[0;36melmo_vectors\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# return average of ELMo features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 967\u001b[1;33m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[0;32m    968\u001b[0m                          run_metadata_ptr)\n\u001b[0;32m    969\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1188\u001b[0m     \u001b[1;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1190\u001b[1;33m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[0;32m   1191\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[0;32m   1192\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1368\u001b[1;33m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[0;32m   1369\u001b[0m                            run_metadata)\n\u001b[0;32m   1370\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1392\u001b[0m                     \u001b[1;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1393\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[1;32m-> 1394\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1396\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Cannot colocate nodes node module_1_apply_default_14/map/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3 (defined at C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow_hub\\native_module.py:615) placed on device No device assignments were active during op 'module_1_apply_default_14/map/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3' creation.  and node module_1_apply_default_14/Reshape (defined at C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow_hub\\native_module.py:615) placed on device Device assignments active during op 'module_1_apply_default_14/Reshape' creation:\n  with tf.device(/device:CPU:0): <C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py:80>\n  with tf.device(/gpu:1): <<ipython-input-19-88410196d8f3>:10> : Cannot merge devices with incompatible types: '/device:GPU:1' and '/device:CPU:0'\n\t [[node module_1_apply_default_14/map/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3 (defined at C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow_hub\\native_module.py:615) ]]Additional information about colocations:No node-device colocations were active during op 'module_1_apply_default_14/map/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3' creation.\nNo device assignments were active during op 'module_1_apply_default_14/map/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3' creation.\nNo node-device colocations were active during op 'module_1_apply_default_14/Reshape' creation.\nDevice assignments active during op 'module_1_apply_default_14/Reshape' creation:\n  with tf.device(/device:CPU:0): <C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py:80>\n  with tf.device(/gpu:1): <<ipython-input-19-88410196d8f3>:10>\n\nOriginal stack trace for 'module_1_apply_default_14/map/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3':\n  File \"c:\\Users\\ngtze\\.vscode\\extensions\\ms-toolsai.jupyter-2021.5.702919634\\pythonFiles\\vscode_datascience_helpers\\..\\pyvsc-run-isolated.py\", line 30, in <module>\n    runpy.run_path(module, run_name=\"__main__\")\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\runpy.py\", line 265, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\runpy.py\", line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"c:\\Users\\ngtze\\.vscode\\extensions\\ms-toolsai.jupyter-2021.5.702919634\\pythonFiles\\vscode_datascience_helpers\\kernel_prewarm_starter.py\", line 31, in <module>\n    runpy.run_module(module, run_name=\"__main__\", alter_sys=False)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\runpy.py\", line 210, in run_module\n    return _run_code(code, {}, init_globals, run_name, mod_spec)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n    app.start()\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 149, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n    handle._run()\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\asyncio\\events.py\", line 81, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n    self.run()\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 543, in execute_request\n    self.do_execute(\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2877, in run_cell\n    result = self._run_cell(\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2923, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3146, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3418, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-19-88410196d8f3>\", line 17, in <module>\n    temp = elmo_vectors(x)\n  File \"<ipython-input-14-35d5d78b2e58>\", line 5, in elmo_vectors\n    embeddings = elmo(x, signature=\"default\", as_dict=True)[\"elmo\"]\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow_hub\\module.py\", line 266, in __call__\n    dict_outputs = self._impl.create_apply_graph(\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow_hub\\native_module.py\", line 615, in create_apply_graph\n    tf.compat.v1.train.import_meta_graph(\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1459, in import_meta_graph\n    return _import_meta_graph_with_return_elements(meta_graph_or_file,\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1480, in _import_meta_graph_with_return_elements\n    meta_graph.import_scoped_meta_graph_with_return_elements(\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\meta_graph.py\", line 799, in import_scoped_meta_graph_with_return_elements\n    imported_return_elements = importer.import_graph_def(\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 538, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py\", line 400, in import_graph_def\n    return _import_graph_def_internal(\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py\", line 513, in _import_graph_def_internal\n    _ProcessNewOps(graph)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py\", line 243, in _ProcessNewOps\n    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3678, in _add_new_tf_operations\n    new_ops = [\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3679, in <listcomp>\n    self._create_op_from_tf_operation(c_op, compute_device=compute_devices)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3561, in _create_op_from_tf_operation\n    ret = Operation(c_op, self)\n  File \"C:\\Users\\ngtze\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1990, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 100\n",
    "\n",
    "try:\n",
    "    elmo_embeddings = pd.read_pickle('./context/elmo_content_embed_{}.pkl'.format(embedding_size))\n",
    "\n",
    "except:\n",
    "    ### Build embeddding\n",
    "    elmo_embeddings = {}\n",
    "   \n",
    "    for para in tqdm(context_para):\n",
    "        words = para.split()\n",
    "        for word in words:\n",
    "            if word in elmo_embeddings.keys():\n",
    "                continue\n",
    "            else:\n",
    "                temp = elmo_vectors(x)\n",
    "                elmo_embeddings[word] = temp[0][:embedding_size]\n",
    "                  \n",
    "    ## Saving BERT Embeddings\n",
    "    with open(\"./context/elmo_content_embed_{}.pkl\".format(embedding_size), \"wb\") as e:\n",
    "        pickle.dump(elmo_embeddings, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": ".venv",
   "display_name": ".venv",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}