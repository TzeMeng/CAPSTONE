{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer, TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting install\n",
      "  Downloading install-1.3.4-py3-none-any.whl (3.1 kB)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\xingy\\anaconda3\\lib\\site-packages (2.3.1)\n",
      "Collecting hub\n",
      "  Downloading hub-1.2.2-py3-none-any.whl (117 kB)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from tensorflow) (1.18.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from tensorflow) (1.32.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from tensorflow) (0.10.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Collecting s3fs==0.4.2\n",
      "  Downloading s3fs-0.4.2-py3-none-any.whl (19 kB)\n",
      "Collecting azure-storage-blob==12.6.0\n",
      "  Downloading azure_storage_blob-12.6.0-py2.py3-none-any.whl (328 kB)\n",
      "Collecting boto3==1.16.39\n",
      "  Downloading boto3-1.16.39-py2.py3-none-any.whl (130 kB)\n",
      "Collecting zarr==2.6.1\n",
      "  Downloading zarr-2.6.1-py3-none-any.whl (132 kB)\n",
      "Collecting cachey==0.2.1\n",
      "  Downloading cachey-0.2.1-py3-none-any.whl (6.4 kB)\n",
      "Collecting fsspec==0.8.5\n",
      "  Downloading fsspec-0.8.5-py3-none-any.whl (98 kB)\n",
      "Collecting psutil>=5.7.3\n",
      "  Downloading psutil-5.8.0-cp37-cp37m-win_amd64.whl (244 kB)\n",
      "Collecting outdated==0.2.0\n",
      "  Downloading outdated-0.2.0.tar.gz (4.0 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from hub) (2.22.0)\n",
      "Collecting pathos>=0.2.2\n",
      "  Downloading pathos-0.2.7-py2.py3-none-any.whl (81 kB)\n",
      "Collecting tqdm==4.54.1\n",
      "  Downloading tqdm-4.54.1-py2.py3-none-any.whl (69 kB)\n",
      "Requirement already satisfied: click<8,>=6.7 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from hub) (7.0)\n",
      "Collecting lz4<4,>=3\n",
      "  Downloading lz4-3.1.3-cp37-cp37m-win_amd64.whl (192 kB)\n",
      "Collecting cloudpickle==1.6.0\n",
      "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
      "Collecting msgpack==1.0.2\n",
      "  Downloading msgpack-1.0.2-cp37-cp37m-win_amd64.whl (68 kB)\n",
      "Collecting Pillow>=8.0.1\n",
      "  Downloading Pillow-8.1.0-cp37-cp37m-win_amd64.whl (2.2 MB)\n",
      "Collecting gcsfs==0.6.2\n",
      "  Downloading gcsfs-0.6.2-py2.py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from protobuf>=3.9.2->tensorflow) (45.2.0.post20200210)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.22.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.3)\n",
      "Collecting botocore>=1.12.91\n",
      "  Downloading botocore-1.20.12-py2.py3-none-any.whl (7.2 MB)\n",
      "Collecting azure-core<2.0.0,>=1.9.0\n",
      "  Downloading azure_core-1.11.0-py2.py3-none-any.whl (127 kB)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from azure-storage-blob==12.6.0->hub) (2.8)\n",
      "Collecting msrest>=0.6.10\n",
      "  Downloading msrest-0.6.21-py2.py3-none-any.whl (85 kB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.4-py2.py3-none-any.whl (69 kB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting fasteners\n",
      "  Downloading fasteners-0.16-py2.py3-none-any.whl (28 kB)\n",
      "Collecting numcodecs>=0.6.4\n",
      "  Downloading numcodecs-0.7.3-cp37-cp37m-win_amd64.whl (687 kB)\n",
      "Collecting asciitree\n",
      "  Downloading asciitree-0.3.3.tar.gz (4.0 kB)\n",
      "Requirement already satisfied: heapdict in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from cachey==0.2.1->hub) (1.0.1)\n",
      "Collecting littleutils\n",
      "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from requests<3,>=2->hub) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from requests<3,>=2->hub) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from requests<3,>=2->hub) (1.25.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from requests<3,>=2->hub) (2.8)\n",
      "Collecting multiprocess>=0.70.11\n",
      "  Downloading multiprocess-0.70.11.1-py37-none-any.whl (108 kB)\n",
      "Collecting dill>=0.3.3\n",
      "  Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
      "Collecting ppft>=1.6.6.3\n",
      "  Downloading ppft-1.6.6.3-py3-none-any.whl (65 kB)\n",
      "Collecting pox>=0.2.9\n",
      "  Downloading pox-0.2.9-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: decorator in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from gcsfs==0.6.2->hub) (4.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (1.5.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from botocore>=1.12.91->s3fs==0.4.2->hub) (2.8.1)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from cryptography>=2.1.4->azure-storage-blob==12.6.0->hub) (1.14.0)\n",
      "Collecting isodate>=0.6.0\n",
      "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\xingy\\anaconda3\\lib\\site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.1.4->azure-storage-blob==12.6.0->hub) (2.19)\n",
      "Building wheels for collected packages: outdated, asciitree, littleutils\n",
      "  Building wheel for outdated (setup.py): started\n",
      "  Building wheel for outdated (setup.py): finished with status 'done'\n",
      "  Created wheel for outdated: filename=outdated-0.2.0-py3-none-any.whl size=4965 sha256=ca0a907d498d121d00ad6ccbf1644ab0ee2387eb51d08870d9783da4aacd8782\n",
      "  Stored in directory: c:\\users\\xingy\\appdata\\local\\pip\\cache\\wheels\\6f\\cd\\a2\\e49170b2cf59e88b952f3414f25a54d9f16f033bded4aaab26\n",
      "  Building wheel for asciitree (setup.py): started\n",
      "  Building wheel for asciitree (setup.py): finished with status 'done'\n",
      "  Created wheel for asciitree: filename=asciitree-0.3.3-py3-none-any.whl size=5041 sha256=ad3ec2a3125afadaaae3335b76a3076d710ecc58a1c049bc2e0c741a88f5eee5\n",
      "  Stored in directory: c:\\users\\xingy\\appdata\\local\\pip\\cache\\wheels\\12\\1c\\38\\0def51e15add93bff3f4bf9c248b94db0839b980b8535e72a0\n",
      "  Building wheel for littleutils (setup.py): started\n",
      "  Building wheel for littleutils (setup.py): finished with status 'done'\n",
      "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7052 sha256=22518e587e869af4ed1c805f1cbf117c3f4f24a87ad6492ffe3ae67c61599f0b\n",
      "  Stored in directory: c:\\users\\xingy\\appdata\\local\\pip\\cache\\wheels\\d6\\64\\cd\\32819b511a488e4993f2fab909a95330289c3f4e0f6ef4676d\n",
      "Successfully built outdated asciitree littleutils\n",
      "Installing collected packages: install, jmespath, botocore, fsspec, s3fs, azure-core, isodate, msrest, azure-storage-blob, s3transfer, boto3, fasteners, numcodecs, asciitree, zarr, cachey, psutil, littleutils, outdated, dill, multiprocess, ppft, pox, pathos, tqdm, lz4, cloudpickle, msgpack, Pillow, gcsfs, hub\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 0.6.2\n",
      "    Uninstalling fsspec-0.6.2:\n",
      "      Successfully uninstalled fsspec-0.6.2\n",
      "  Attempting uninstall: psutil\n",
      "    Found existing installation: psutil 5.6.7\n",
      "    Uninstalling psutil-5.6.7:\n",
      "      Successfully uninstalled psutil-5.6.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: spyder 4.0.1 requires pyqt5<5.13; python_version >= \"3\", which is not installed.\n",
      "ERROR: spyder 4.0.1 requires pyqtwebengine<5.13; python_version >= \"3\", which is not installed.\n",
      "ERROR: pytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\n",
      "ERROR: pytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\n",
      "ERROR: boto3 1.16.39 has requirement botocore<1.20.0,>=1.19.39, but you'll have botocore 1.20.12 which is incompatible.\n",
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'c:\\\\users\\\\xingy\\\\anaconda3\\\\lib\\\\site-packages\\\\~sutil\\\\_psutil_windows.cp37-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install install tensorflow hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample:\n",
    "    def __init__(self, question, context, start_char_idx=None, answer_text=None, all_answers=None):\n",
    "        self.question = question\n",
    "        self.context = context\n",
    "        self.start_char_idx = start_char_idx\n",
    "        self.answer_text = answer_text\n",
    "        self.all_answers = all_answers\n",
    "        self.skip = False\n",
    "        self.start_token_idx = -1\n",
    "        self.end_token_idx = -1\n",
    "\n",
    "    def preprocess(self):\n",
    "        # clean context and question\n",
    "        context = \" \".join(str(self.context).split())\n",
    "        question = \" \".join(str(self.question).split())\n",
    "        # tokenize context and question\n",
    "        tokenized_context = tokenizer.encode(context)\n",
    "        tokenized_question = tokenizer.encode(question)\n",
    "        # if this is validation or training sample, preprocess answer\n",
    "        if self.answer_text is not None:\n",
    "            answer = \" \".join(str(self.answer_text).split())\n",
    "            # check if end character index is in the context\n",
    "            end_char_idx = self.start_char_idx + len(answer)\n",
    "            if end_char_idx >= len(context):\n",
    "                self.skip = True\n",
    "                return\n",
    "            # mark all the character indexes in context that are also in answer     \n",
    "            is_char_in_ans = [0] * len(context)\n",
    "            for idx in range(self.start_char_idx, end_char_idx):\n",
    "                is_char_in_ans[idx] = 1\n",
    "            ans_token_idx = []\n",
    "            # find all the tokens that are in the answers\n",
    "            for idx, (start, end) in enumerate(tokenized_context.offsets):\n",
    "                if sum(is_char_in_ans[start:end]) > 0:\n",
    "                    ans_token_idx.append(idx)\n",
    "            if len(ans_token_idx) == 0:\n",
    "                self.skip = True\n",
    "                return\n",
    "            # get start and end token indexes\n",
    "            self.start_token_idx = ans_token_idx[0]\n",
    "            self.end_token_idx = ans_token_idx[-1]\n",
    "        # create inputs as usual\n",
    "        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n",
    "        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(tokenized_question.ids[1:])\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        padding_length = max_seq_length - len(input_ids)\n",
    "        # add padding if necessary\n",
    "        if padding_length > 0:\n",
    "            input_ids = input_ids + ([0] * padding_length)\n",
    "            attention_mask = attention_mask + ([0] * padding_length)\n",
    "            token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        elif padding_length < 0:\n",
    "            self.skip = True\n",
    "            return\n",
    "        self.input_word_ids = input_ids\n",
    "        self.input_type_ids = token_type_ids\n",
    "        self.input_mask = attention_mask\n",
    "        self.context_token_to_char = tokenized_context.offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_squad_examples(raw_data):\n",
    "    squad_examples = []\n",
    "    for item in raw_data[\"data\"]:\n",
    "        for para in item[\"paragraphs\"]:\n",
    "            context = para[\"context\"]\n",
    "            for qa in para[\"qas\"]:\n",
    "                question = qa[\"question\"]\n",
    "                if \"answers\" in qa:\n",
    "                    answer_text = qa[\"answers\"][0][\"text\"]\n",
    "                    all_answers = [_[\"text\"] for _ in qa[\"answers\"]]\n",
    "                    start_char_idx = qa[\"answers\"][0][\"answer_start\"]\n",
    "                    squad_eg = Sample(question, context, start_char_idx, answer_text, all_answers)\n",
    "                else:\n",
    "                    squad_eg = Sample(question, context)\n",
    "                squad_eg.preprocess()\n",
    "                squad_examples.append(squad_eg)\n",
    "    return squad_examples\n",
    "\n",
    "\n",
    "def create_inputs_targets(squad_examples):\n",
    "    dataset_dict = {\n",
    "        \"input_word_ids\": [],\n",
    "        \"input_type_ids\": [],\n",
    "        \"input_mask\": [],\n",
    "        \"start_token_idx\": [],\n",
    "        \"end_token_idx\": [],\n",
    "    }\n",
    "    for item in squad_examples:\n",
    "        if item.skip == False:\n",
    "            for key in dataset_dict:\n",
    "                dataset_dict[key].append(getattr(item, key))\n",
    "    for key in dataset_dict:\n",
    "        dataset_dict[key] = np.array(dataset_dict[key])\n",
    "    x = [dataset_dict[\"input_word_ids\"],\n",
    "         dataset_dict[\"input_mask\"],\n",
    "         dataset_dict[\"input_type_ids\"]]\n",
    "    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationCallback(keras.callbacks.Callback):\n",
    "\n",
    "    def normalize_text(self, text):\n",
    "        # convert to lower case\n",
    "        text = text.lower()\n",
    "        # remove redundant whitespaces\n",
    "        text = \"\".join(ch for ch in text if ch not in set(string.punctuation))\n",
    "        # remove articles\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        text = re.sub(regex, \" \", text)\n",
    "        text = \" \".join(text.split())\n",
    "        return text\n",
    "\n",
    "    def __init__(self, x_eval, y_eval):\n",
    "        self.x_eval = x_eval\n",
    "        self.y_eval = y_eval\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # get the offsets of the first and last tokens of predicted answers\n",
    "        pred_start, pred_end = self.model.predict(self.x_eval)\n",
    "        count = 0\n",
    "        eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\n",
    "        # for every pair of offsets\n",
    "        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
    "            # take the required Sample object with the ground-truth answers in it\n",
    "            squad_eg = eval_examples_no_skip[idx]\n",
    "            # use offsets to get back the span of text corresponding to\n",
    "            # our predicted first and last tokens\n",
    "            offsets = squad_eg.context_token_to_char\n",
    "            start = np.argmax(start)\n",
    "            end = np.argmax(end)\n",
    "            if start >= len(offsets):\n",
    "                continue\n",
    "            pred_char_start = offsets[start][0]\n",
    "            if end < len(offsets):\n",
    "                pred_char_end = offsets[end][1]\n",
    "                pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
    "            else:\n",
    "                pred_ans = squad_eg.context[pred_char_start:]\n",
    "            normalized_pred_ans = self.normalize_text(pred_ans)\n",
    "            # clean the real answers\n",
    "            normalized_true_ans = [self.normalize_text(_) for _ in squad_eg.all_answers]\n",
    "            # check if the predicted answer is in an array of the ground-truth answers\n",
    "            if normalized_pred_ans in normalized_true_ans:\n",
    "                count += 1\n",
    "        acc = count / len(self.y_eval[0])\n",
    "        print(f\"\\nepoch={epoch + 1}, exact match score={acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = keras.utils.get_file(\"train.json\", \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\")\n",
    "eval_path = keras.utils.get_file(\"eval.json\", \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\")\n",
    "with open(train_path) as f: raw_train_data = json.load(f)\n",
    "with open(eval_path) as f: raw_eval_data = json.load(f)\n",
    "max_seq_length = 384\n",
    "\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_word_ids')\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_mask')\n",
    "input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_type_ids')\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\", trainable=True)\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy().decode(\"utf-8\")\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertWordPieceTokenizer(vocab=vocab_file, lowercase=True)\n",
    "train_squad_examples = create_squad_examples(raw_train_data)\n",
    "x_train, y_train = create_inputs_targets(train_squad_examples)\n",
    "print(f\"{len(train_squad_examples)} training points created.\")\n",
    "eval_squad_examples = create_squad_examples(raw_eval_data)\n",
    "x_eval, y_eval = create_inputs_targets(eval_squad_examples)\n",
    "print(f\"{len(eval_squad_examples)} evaluation points created.\")\n",
    "start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(sequence_output)\n",
    "start_logits = layers.Flatten()(start_logits)\n",
    "end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(sequence_output)\n",
    "end_logits = layers.Flatten()(end_logits)\n",
    "start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n",
    "end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n",
    "model = keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=[start_probs, end_probs])\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "optimizer = keras.optimizers.Adam(lr=1e-5, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "model.compile(optimizer=optimizer, loss=[loss, loss])\n",
    "model.summary()\n",
    "model.fit(x_train, y_train, epochs=2, batch_size=8, callbacks=[ValidationCallback(x_eval, y_eval)])\n",
    "model.save_weights(\"./weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
