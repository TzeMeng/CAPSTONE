{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "opened-advance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class Vocabulary(object):\n",
    "    '''\n",
    "    A token vocabulary.  Holds a map from token to ids and provides\n",
    "    a method for encoding text to a sequence of ids.\n",
    "    '''\n",
    "    def __init__(self, filename, validate_file=False):\n",
    "        '''\n",
    "        filename = the vocabulary file.  It is a flat text file with one\n",
    "            (normalized) token per line.  In addition, the file should also\n",
    "            contain the special tokens <S>, </S>, <UNK> (case sensitive).\n",
    "        '''\n",
    "        self._id_to_word = []\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._bos = -1\n",
    "        self._eos = -1\n",
    "\n",
    "        with open(filename) as f:\n",
    "            idx = 0\n",
    "            for line in f:\n",
    "                word_name = line.strip()\n",
    "                if word_name == '<S>':\n",
    "                    self._bos = idx\n",
    "                elif word_name == '</S>':\n",
    "                    self._eos = idx\n",
    "                elif word_name == '<UNK>':\n",
    "                    self._unk = idx\n",
    "                if word_name == '!!!MAXTERMID':\n",
    "                    continue\n",
    "\n",
    "                self._id_to_word.append(word_name)\n",
    "                self._word_to_id[word_name] = idx\n",
    "                idx += 1\n",
    "\n",
    "        # check to ensure file has special tokens\n",
    "        if validate_file:\n",
    "            if self._bos == -1 or self._eos == -1 or self._unk == -1:\n",
    "                raise ValueError(\"Ensure the vocabulary file has \"\n",
    "                                 \"<S>, </S>, <UNK> tokens\")\n",
    "\n",
    "    @property\n",
    "    def bos(self):\n",
    "        return self._bos\n",
    "\n",
    "    @property\n",
    "    def eos(self):\n",
    "        return self._eos\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self._id_to_word)\n",
    "\n",
    "    def word_to_id(self, word):\n",
    "        if word in self._word_to_id:\n",
    "            return self._word_to_id[word]\n",
    "        return self.unk\n",
    "\n",
    "    def id_to_word(self, cur_id):\n",
    "        return self._id_to_word[cur_id]\n",
    "\n",
    "    def decode(self, cur_ids):\n",
    "        \"\"\"Convert a list of ids to a sentence, with space inserted.\"\"\"\n",
    "        return ' '.join([self.id_to_word(cur_id) for cur_id in cur_ids])\n",
    "\n",
    "    def encode(self, sentence, reverse=False, split=True):\n",
    "        \"\"\"Convert a sentence to a list of ids, with special tokens added.\n",
    "        Sentence is a single string with tokens separated by whitespace.\n",
    "        If reverse, then the sentence is assumed to be reversed, and\n",
    "            this method will swap the BOS/EOS tokens appropriately.\"\"\"\n",
    "\n",
    "        if split:\n",
    "            word_ids = [\n",
    "                self.word_to_id(cur_word) for cur_word in sentence.split()\n",
    "            ]\n",
    "        else:\n",
    "            word_ids = [self.word_to_id(cur_word) for cur_word in sentence]\n",
    "\n",
    "        if reverse:\n",
    "            return np.array([self.eos] + word_ids + [self.bos], dtype=np.int32)\n",
    "        else:\n",
    "            return np.array([self.bos] + word_ids + [self.eos], dtype=np.int32)\n",
    "\n",
    "\n",
    "class UnicodeCharsVocabulary(Vocabulary):\n",
    "    \"\"\"Vocabulary containing character-level and word level information.\n",
    "    Has a word vocabulary that is used to lookup word ids and\n",
    "    a character id that is used to map words to arrays of character ids.\n",
    "    The character ids are defined by ord(c) for c in word.encode('utf-8')\n",
    "    This limits the total number of possible char ids to 256.\n",
    "    To this we add 5 additional special ids: begin sentence, end sentence,\n",
    "        begin word, end word and padding.\n",
    "    WARNING: for prediction, we add +1 to the output ids from this\n",
    "    class to create a special padding id (=0).  As a result, we suggest\n",
    "    you use the `Batcher`, `TokenBatcher`, and `LMDataset` classes instead\n",
    "    of this lower level class.  If you are using this lower level class,\n",
    "    then be sure to add the +1 appropriately, otherwise embeddings computed\n",
    "    from the pre-trained model will be useless.\n",
    "    \"\"\"\n",
    "    def __init__(self, filename, max_word_length, **kwargs):\n",
    "        super(UnicodeCharsVocabulary, self).__init__(filename, **kwargs)\n",
    "        self._max_word_length = max_word_length\n",
    "\n",
    "        # char ids 0-255 come from utf-8 encoding bytes\n",
    "        # assign 256-300 to special chars\n",
    "        self.bos_char = 256  # <begin sentence>\n",
    "        self.eos_char = 257  # <end sentence>\n",
    "        self.bow_char = 258  # <begin word>\n",
    "        self.eow_char = 259  # <end word>\n",
    "        self.pad_char = 260 # <padding>\n",
    "\n",
    "        num_words = len(self._id_to_word)\n",
    "\n",
    "        self._word_char_ids = np.zeros([num_words, max_word_length],\n",
    "            dtype=np.int32)\n",
    "\n",
    "        # the charcter representation of the begin/end of sentence characters\n",
    "        def _make_bos_eos(c):\n",
    "            r = np.zeros([self.max_word_length], dtype=np.int32)\n",
    "            r[:] = self.pad_char\n",
    "            r[0] = self.bow_char\n",
    "            r[1] = c\n",
    "            r[2] = self.eow_char\n",
    "            return r\n",
    "        self.bos_chars = _make_bos_eos(self.bos_char)\n",
    "        self.eos_chars = _make_bos_eos(self.eos_char)\n",
    "\n",
    "        for i, word in enumerate(self._id_to_word):\n",
    "            self._word_char_ids[i] = self._convert_word_to_char_ids(word)\n",
    "\n",
    "        self._word_char_ids[self.bos] = self.bos_chars\n",
    "        self._word_char_ids[self.eos] = self.eos_chars\n",
    "        # TODO: properly handle <UNK>\n",
    "\n",
    "    @property\n",
    "    def word_char_ids(self):\n",
    "        return self._word_char_ids\n",
    "\n",
    "    @property\n",
    "    def max_word_length(self):\n",
    "        return self._max_word_length\n",
    "\n",
    "    def _convert_word_to_char_ids(self, word):\n",
    "        code = np.zeros([self.max_word_length], dtype=np.int32)\n",
    "        code[:] = self.pad_char\n",
    "\n",
    "        word_encoded = word.encode('utf-8', 'ignore')[:(self.max_word_length-2)]\n",
    "        code[0] = self.bow_char\n",
    "        for k, chr_id in enumerate(word_encoded, start=1):\n",
    "            code[k] = chr_id\n",
    "        code[len(word_encoded) + 1] = self.eow_char\n",
    "\n",
    "        return code\n",
    "\n",
    "    def word_to_char_ids(self, word):\n",
    "        if word in self._word_to_id:\n",
    "            return self._word_char_ids[self._word_to_id[word]]\n",
    "        else:\n",
    "            return self._convert_word_to_char_ids(word)\n",
    "\n",
    "    def encode_chars(self, sentence, reverse=False, split=True):\n",
    "        '''\n",
    "        Encode the sentence as a white space delimited string of tokens.\n",
    "        '''\n",
    "        if split:\n",
    "            chars_ids = [self.word_to_char_ids(cur_word)\n",
    "                     for cur_word in sentence.split()]\n",
    "        else:\n",
    "            chars_ids = [self.word_to_char_ids(cur_word)\n",
    "                     for cur_word in sentence]\n",
    "        if reverse:\n",
    "            return np.vstack([self.eos_chars] + chars_ids + [self.bos_chars])\n",
    "        else:\n",
    "            return np.vstack([self.bos_chars] + chars_ids + [self.eos_chars])\n",
    "\n",
    "\n",
    "class Batcher(object):\n",
    "    ''' \n",
    "    Batch sentences of tokenized text into character id matrices.\n",
    "    '''\n",
    "    def __init__(self, lm_vocab_file: str, max_token_length: int):\n",
    "        '''\n",
    "        lm_vocab_file = the language model vocabulary file (one line per\n",
    "            token)\n",
    "        max_token_length = the maximum number of characters in each token\n",
    "        '''\n",
    "        self._lm_vocab = UnicodeCharsVocabulary(\n",
    "            lm_vocab_file, max_token_length\n",
    "        )\n",
    "        self._max_token_length = max_token_length\n",
    "\n",
    "    def batch_sentences(self, sentences: List[List[str]]):\n",
    "        '''\n",
    "        Batch the sentences as character ids\n",
    "        Each sentence is a list of tokens without <s> or </s>, e.g.\n",
    "        [['The', 'first', 'sentence', '.'], ['Second', '.']]\n",
    "        '''\n",
    "        n_sentences = len(sentences)\n",
    "        max_length = max(len(sentence) for sentence in sentences) + 2\n",
    "\n",
    "        X_char_ids = np.zeros(\n",
    "            (n_sentences, max_length, self._max_token_length),\n",
    "            dtype=np.int64\n",
    "        )\n",
    "\n",
    "        for k, sent in enumerate(sentences):\n",
    "            length = len(sent) + 2\n",
    "            char_ids_without_mask = self._lm_vocab.encode_chars(\n",
    "                sent, split=False)\n",
    "            # add one so that 0 is the mask value\n",
    "            X_char_ids[k, :length, :] = char_ids_without_mask + 1\n",
    "\n",
    "        return X_char_ids\n",
    "\n",
    "\n",
    "class TokenBatcher(object):\n",
    "    ''' \n",
    "    Batch sentences of tokenized text into token id matrices.\n",
    "    '''\n",
    "    def __init__(self, lm_vocab_file: str):\n",
    "        '''\n",
    "        lm_vocab_file = the language model vocabulary file (one line per\n",
    "            token)\n",
    "        '''\n",
    "        self._lm_vocab = Vocabulary(lm_vocab_file)\n",
    "\n",
    "    def batch_sentences(self, sentences: List[List[str]]):\n",
    "        '''\n",
    "        Batch the sentences as character ids\n",
    "        Each sentence is a list of tokens without <s> or </s>, e.g.\n",
    "        [['The', 'first', 'sentence', '.'], ['Second', '.']]\n",
    "        '''\n",
    "        n_sentences = len(sentences)\n",
    "        max_length = max(len(sentence) for sentence in sentences) + 2\n",
    "\n",
    "        X_ids = np.zeros((n_sentences, max_length), dtype=np.int64)\n",
    "\n",
    "        for k, sent in enumerate(sentences):\n",
    "            length = len(sent) + 2\n",
    "            ids_without_mask = self._lm_vocab.encode(sent, split=False)\n",
    "            # add one so that 0 is the mask value\n",
    "            X_ids[k, :length] = ids_without_mask + 1\n",
    "\n",
    "        return X_ids\n",
    "\n",
    "\n",
    "##### for training\n",
    "def _get_batch(generator, batch_size, num_steps, max_word_length):\n",
    "    \"\"\"Read batches of input.\"\"\"\n",
    "    cur_stream = [None] * batch_size\n",
    "\n",
    "    no_more_data = False\n",
    "    while True:\n",
    "        inputs = np.zeros([batch_size, num_steps], np.int32)\n",
    "        if max_word_length is not None:\n",
    "            char_inputs = np.zeros([batch_size, num_steps, max_word_length],\n",
    "                                np.int32)\n",
    "        else:\n",
    "            char_inputs = None\n",
    "        targets = np.zeros([batch_size, num_steps], np.int32)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            cur_pos = 0\n",
    "\n",
    "            while cur_pos < num_steps:\n",
    "                if cur_stream[i] is None or len(cur_stream[i][0]) <= 1:\n",
    "                    try:\n",
    "                        cur_stream[i] = list(next(generator))\n",
    "                    except StopIteration:\n",
    "                        # No more data, exhaust current streams and quit\n",
    "                        no_more_data = True\n",
    "                        break\n",
    "\n",
    "                how_many = min(len(cur_stream[i][0]) - 1, num_steps - cur_pos)\n",
    "                next_pos = cur_pos + how_many\n",
    "\n",
    "                inputs[i, cur_pos:next_pos] = cur_stream[i][0][:how_many]\n",
    "                if max_word_length is not None:\n",
    "                    char_inputs[i, cur_pos:next_pos] = cur_stream[i][1][\n",
    "                                                                    :how_many]\n",
    "                targets[i, cur_pos:next_pos] = cur_stream[i][0][1:how_many+1]\n",
    "\n",
    "                cur_pos = next_pos\n",
    "\n",
    "                cur_stream[i][0] = cur_stream[i][0][how_many:]\n",
    "                if max_word_length is not None:\n",
    "                    cur_stream[i][1] = cur_stream[i][1][how_many:]\n",
    "\n",
    "        if no_more_data:\n",
    "            # There is no more data.  Note: this will not return data\n",
    "            # for the incomplete batch\n",
    "            break\n",
    "\n",
    "        X = {'token_ids': inputs, 'tokens_characters': char_inputs,\n",
    "                 'next_token_id': targets}\n",
    "\n",
    "        yield X\n",
    "\n",
    "class LMDataset(object):\n",
    "    \"\"\"\n",
    "    Hold a language model dataset.\n",
    "    A dataset is a list of tokenized files.  Each file contains one sentence\n",
    "        per line.  Each sentence is pre-tokenized and white space joined.\n",
    "    \"\"\"\n",
    "    def __init__(self, filepattern, vocab, reverse=False, test=False,\n",
    "                 shuffle_on_load=False):\n",
    "        '''\n",
    "        filepattern = a glob string that specifies the list of files.\n",
    "        vocab = an instance of Vocabulary or UnicodeCharsVocabulary\n",
    "        reverse = if True, then iterate over tokens in each sentence in reverse\n",
    "        test = if True, then iterate through all data once then stop.\n",
    "            Otherwise, iterate forever.\n",
    "        shuffle_on_load = if True, then shuffle the sentences after loading.\n",
    "        '''\n",
    "        self._vocab = vocab\n",
    "        self._all_shards = glob.glob(filepattern)\n",
    "        print('Found %d shards at %s' % (len(self._all_shards), filepattern))\n",
    "        self._shards_to_choose = []\n",
    "\n",
    "        self._reverse = reverse\n",
    "        self._test = test\n",
    "        self._shuffle_on_load = shuffle_on_load\n",
    "        self._use_char_inputs = hasattr(vocab, 'encode_chars')\n",
    "\n",
    "        self._ids = self._load_random_shard()\n",
    "\n",
    "    def _choose_random_shard(self):\n",
    "        if len(self._shards_to_choose) == 0:\n",
    "            self._shards_to_choose = list(self._all_shards)\n",
    "            random.shuffle(self._shards_to_choose)\n",
    "        shard_name = self._shards_to_choose.pop()\n",
    "        return shard_name\n",
    "\n",
    "    def _load_random_shard(self):\n",
    "        \"\"\"Randomly select a file and read it.\"\"\"\n",
    "        if self._test:\n",
    "            if len(self._all_shards) == 0:\n",
    "                # we've loaded all the data\n",
    "                # this will propogate up to the generator in get_batch\n",
    "                # and stop iterating\n",
    "                raise StopIteration\n",
    "            else:\n",
    "                shard_name = self._all_shards.pop()\n",
    "        else:\n",
    "            # just pick a random shard\n",
    "            shard_name = self._choose_random_shard()\n",
    "\n",
    "        ids = self._load_shard(shard_name)\n",
    "        self._i = 0\n",
    "        self._nids = len(ids)\n",
    "        return ids\n",
    "\n",
    "    def _load_shard(self, shard_name):\n",
    "        \"\"\"Read one file and convert to ids.\n",
    "        Args:\n",
    "            shard_name: file path.\n",
    "        Returns:\n",
    "            list of (id, char_id) tuples.\n",
    "        \"\"\"\n",
    "        print('Loading data from: %s' % shard_name)\n",
    "        with open(shard_name) as f:\n",
    "            sentences_raw = f.readlines()\n",
    "\n",
    "        if self._reverse:\n",
    "            sentences = []\n",
    "            for sentence in sentences_raw:\n",
    "                splitted = sentence.split()\n",
    "                splitted.reverse()\n",
    "                sentences.append(' '.join(splitted))\n",
    "        else:\n",
    "            sentences = sentences_raw\n",
    "\n",
    "        if self._shuffle_on_load:\n",
    "            random.shuffle(sentences)\n",
    "\n",
    "        ids = [self.vocab.encode(sentence, self._reverse)\n",
    "               for sentence in sentences]\n",
    "        if self._use_char_inputs:\n",
    "            chars_ids = [self.vocab.encode_chars(sentence, self._reverse)\n",
    "                     for sentence in sentences]\n",
    "        else:\n",
    "            chars_ids = [None] * len(ids)\n",
    "\n",
    "        print('Loaded %d sentences.' % len(ids))\n",
    "        print('Finished loading')\n",
    "        return list(zip(ids, chars_ids))\n",
    "\n",
    "    def get_sentence(self):\n",
    "        while True:\n",
    "            if self._i == self._nids:\n",
    "                self._ids = self._load_random_shard()\n",
    "            ret = self._ids[self._i]\n",
    "            self._i += 1\n",
    "            yield ret\n",
    "\n",
    "    @property\n",
    "    def max_word_length(self):\n",
    "        if self._use_char_inputs:\n",
    "            return self._vocab.max_word_length\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def iter_batches(self, batch_size, num_steps):\n",
    "        for X in _get_batch(self.get_sentence(), batch_size, num_steps,\n",
    "                           self.max_word_length):\n",
    "\n",
    "            # token_ids = (batch_size, num_steps)\n",
    "            # char_inputs = (batch_size, num_steps, 50) of character ids\n",
    "            # targets = word ID of next word (batch_size, num_steps)\n",
    "            yield X\n",
    "\n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return self._vocab\n",
    "\n",
    "class BidirectionalLMDataset(object):\n",
    "    def __init__(self, filepattern, vocab, test=False, shuffle_on_load=False):\n",
    "        '''\n",
    "        bidirectional version of LMDataset\n",
    "        '''\n",
    "        self._data_forward = LMDataset(\n",
    "            filepattern, vocab, reverse=False, test=test,\n",
    "            shuffle_on_load=shuffle_on_load)\n",
    "        self._data_reverse = LMDataset(\n",
    "            filepattern, vocab, reverse=True, test=test,\n",
    "            shuffle_on_load=shuffle_on_load)\n",
    "\n",
    "    def iter_batches(self, batch_size, num_steps):\n",
    "        max_word_length = self._data_forward.max_word_length\n",
    "\n",
    "        for X, Xr in zip(\n",
    "            _get_batch(self._data_forward.get_sentence(), batch_size,\n",
    "                      num_steps, max_word_length),\n",
    "            _get_batch(self._data_reverse.get_sentence(), batch_size,\n",
    "                      num_steps, max_word_length)\n",
    "            ):\n",
    "\n",
    "            for k, v in Xr.items():\n",
    "                X[k + '_reverse'] = v\n",
    "\n",
    "            yield X\n",
    "\n",
    "\n",
    "class InvalidNumberOfCharacters(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-bridge",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
