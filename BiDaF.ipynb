{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "729258264f08065d3a2b15fd5eadee095516ae7ef886559b46c40377df310956"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.utils import to_categorical\n",
    "import inflect\n",
    "import spacy\n",
    "from livelossplot import PlotLossesKeras\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint,EarlyStopping,CSVLogger\n",
    "from keras.layers import AveragePooling1D,TimeDistributed,Bidirectional\n",
    "from keras.layers import MaxPooling1D,RepeatVector,Concatenate\n",
    "from keras.layers import Activation,CuDNNGRU,Multiply\n",
    "from keras.layers import Dropout,Flatten,Permute\n",
    "from keras.layers import Reshape,Lambda,Dense\n",
    "from keras.layers import Input,GRU,Dot,add\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numba\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                             context  contextID\n",
       "0  North Carolina consists of three main geograph...          1\n",
       "1  The coastal plain transitions to the Piedmont ...          2\n",
       "2  The western section of the state is part of th...          3\n",
       "3  The climate of the coastal plain is influenced...          4\n",
       "4  The Atlantic Ocean has less influence on the c...          5"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>context</th>\n      <th>contextID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>North Carolina consists of three main geograph...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The coastal plain transitions to the Piedmont ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The western section of the state is part of th...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The climate of the coastal plain is influenced...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The Atlantic Ocean has less influence on the c...</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "file_name_train =  'data/train/train_context.xlsx'\n",
    "\n",
    "train_context = pd.read_excel(io=file_name_train)\n",
    "train_context.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"BiDAF/train_context.txt\", \"rb\") as myFile:\n",
    "        c_corpus = pickle.load(myFile)\n",
    "except:\n",
    "    filters = '!\\'\"#$%&()*+â€”,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    c_corpus = [text_to_word_sequence(train_context['context'][i],filters=filters,lower=False) for i in range(len(train_context))]\n",
    "    \n",
    "    with open(\"BiDAF/train_context.txt\", \"wb\") as myFile:\n",
    "        pickle.dump(c_corpus, myFile)\n",
    "\n",
    "counter = len(c_corpus)"
   ]
  },
  {
   "source": [
    "### Using spaCy to lemmatize"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"BiDAF/lemmatize_context.txt\", \"rb\") as myFile:\n",
    "        c_corpus_lemm = pickle.load(myFile)\n",
    "\n",
    "except:\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    @numba.jit\n",
    "    def lemmatize(word_seq,nlp):\n",
    "        temp = []\n",
    "        for token in word_seq:\n",
    "            temp.append(nlp(token)[0].lemma_)\n",
    "        return temp\n",
    "\n",
    "    c_corpus_lemm = []\n",
    "    start_time = time.time()\n",
    "    for i in range(len(c_corpus)):\n",
    "        c_corpus_lemm.append(lemmatize(c_corpus[i], nlp))\n",
    "        if i%100 == 0:\n",
    "            print(\"Steps {0} out of {1}\".format(i, len(c_corpus)))\n",
    "            print(\"--- %s seconds ---\\n\" % (time.time() - start_time))\n",
    "    print(\"Done !! ----{0}\".format(time.time() - start_time))\n",
    "    \n",
    "    with open(\"BiDAF/lemmatize_context.txt\", \"wb\") as myFile:\n",
    "        pickle.dump(c_corpus_lemm, myFile)"
   ]
  },
  {
   "source": [
    "Embedding Using GLoVe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"BiDAF/embedding.txt\", \"rb\") as myFile:\n",
    "        embeddings_dict = pickle.load(myFile)\n",
    "\n",
    "except: \n",
    "    #Create embedding _dict that contains the vectors for each word\n",
    "    embeddings_dict = {}\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "    with open(\"glove.6B/glove.6B.50d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector\n",
    "    \n",
    "    with open(\"BiDAF/embedding.txt\", \"wb\") as myFile:\n",
    "        pickle.dump(embeddings_dict, myFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"BiDAF/embedded_context.txt\", \"rb\") as myFile:\n",
    "        embedded_context = pickle.load(myFile)\n",
    "\n",
    "except:\n",
    "    embedded_context = []\n",
    "    missed = []\n",
    "    \n",
    "    for para in c_corpus:\n",
    "        embedded_para = []\n",
    "        for word in para:\n",
    "            \n",
    "            #Cap words cannot be embedded\n",
    "            word = word.lower()\n",
    "\n",
    "            # Range of values cannot be embedded (eg. '11-14'), need to convert to '11 to 14'\n",
    "            try:\n",
    "                embedded_para.append(embeddings_dict[word])\n",
    "            except:\n",
    "                if word not in missed:\n",
    "                    missed.append(word)\n",
    "        embedded_context.append(embedded_para)\n",
    "\n",
    "    with open(\"BiDAF/embedded_context.txt\", \"wb\") as myFile:\n",
    "        pickle.dump(embedded_context, myFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}